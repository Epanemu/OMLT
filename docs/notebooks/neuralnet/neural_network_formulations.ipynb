{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using Neural Network Formulations in OMLT\n",
    "\n",
    "In this notebook we show how OMLT can be used to build different optimization formulations of neural networks within Pyomo. It specifically demonstrates the following examples:<br>\n",
    "1.) A neural network with smooth sigmoid activation functions represented using full-space and reduced-space formulations <br>\n",
    "2.) A neural network with non-smooth ReLU activation functions represented using complementarity and mixed integer formulations <br>\n",
    "3.) A neural network with mixed ReLU and sigmoid activation functions represented using complementarity (for ReLU) and full-space (for sigmoid) formulations <br>\n",
    "<br>\n",
    "After building the OMLT formulations, we minimize each representation of the function and compare the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Library Setup\n",
    "This notebook assumes you have a working Tensorflow environment in addition to necessary Python packages described here. We use Keras to train neural networks of interest for our example which requires the Python Tensorflow package. The neural networks are then formulated in Pyomo using OMLT which therefore requires working Pyomo and OMLT installations.\n",
    "\n",
    "The required Python libraries used this notebook are as follows: <br>\n",
    "- `pandas`: used for data import and management <br>\n",
    "- `matplotlib`: used for plotting the results in this example\n",
    "- `tensorflow`: the machine learning language we use to train our neural network\n",
    "- `pyomo`: the algebraic modeling language for Python, it is used to define the optimization model passed to the solver\n",
    "- `onnx`: used to express trained neural network models\n",
    "- `omlt`: The package this notebook demonstates. OMLT can formulate machine learning models (such as neural networks) within Pyomo\n",
    "\n",
    "**NOTE:** This notebook also assumes you have a working MIP solver executable (e.g., CBC, Gurobi) to solve optimization problems in Pyomo. The open-source solvers CBC and IPOPT are called by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Start by importing the following libraries\n",
    "#data manipulation and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc('font', size=24)\n",
    "plt.rc('axes', titlesize=24)\n",
    "\n",
    "#tensorflow objects\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#pyomo for optimization\n",
    "import pyomo.environ as pyo\n",
    "\n",
    "#omlt for interfacing our neural network with pyomo\n",
    "from omlt import OmltBlock\n",
    "from omlt.neuralnet import NetworkDefinition, FullSpaceNNFormulation, \\\n",
    "FullSpaceSmoothNNFormulation, ReducedSpaceSmoothNNFormulation, ReluBigMFormulation,\\\n",
    "ReluComplementarityFormulation, ReluPartitionFormulation\n",
    "from omlt.neuralnet.activations import ComplementarityReLUActivation\n",
    "from omlt.io.keras import keras_reader\n",
    "import omlt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We begin by training neural networks that learn from data given the following imported dataframe. In practice, this data could represent the output of a simulation, real sensor measurements, or some other external data source. The data contains a single input `x` and a single output `y` and contains 10,000 total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/sin_quadratic.csv\",index_col=[0]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data we use for training is plotted below (on the left figure). We also scale the training data to a mean of zero with unit standard deviation. The scaled inputs and outputs are added to the dataframe and plotted next to the original data values (on the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#retrieve input 'x' and output 'y' from the dataframe\n",
    "x = df[\"x\"]\n",
    "y = df[\"y\"]\n",
    "\n",
    "#calculate mean and standard deviation, add scaled 'x' and scaled 'y' to the dataframe\n",
    "mean_data = df.mean(axis=0)\n",
    "std_data = df.std(axis=0)\n",
    "df[\"x_scaled\"] = (df['x'] - mean_data['x']) / std_data['x']\n",
    "df[\"y_scaled\"] = (df['y'] - mean_data['y']) / std_data['y']\n",
    "\n",
    "#create plots for unscaled and scaled data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,8))\n",
    "\n",
    "ax1.plot(x, y)\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\");\n",
    "ax1.set_title(\"Training Data\")\n",
    "\n",
    "ax2.plot(df[\"x_scaled\"], df[\"y_scaled\"])\n",
    "ax2.set_xlabel(\"x_scaled\")\n",
    "ax2.set_ylabel(\"y_scaled\");\n",
    "ax2.set_title(\"Scaled Training Data\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the Neural Networks\n",
    "After importing the dataset we use Tensorflow (with Keras) to train three neural network models. Each neural network contains 2 layers with 100 nodes per layer with a single output layer. <br>\n",
    "1.) The first network (`nn1`) uses sigmoid activation functions for both layers.<br>\n",
    "2.) The second network (`nn2`) uses ReLU activations<br>\n",
    "3.) The last network (`nn3`) mixes ReLU and sigmoid activation functions. The first layer is sigmoid, the second layer is ReLU. <br>\n",
    "We use the ADAM optimizer and train the first two neural networks for 50 epochs. We train `nn3` for 150 epochs since we observe difficulty obtaining a good fit with the mixed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#sigmoid neural network\n",
    "nn1 = Sequential(name='sin_wave_sigmoid')\n",
    "nn1.add(Input(1))\n",
    "nn1.add(Dense(50, activation='sigmoid'))\n",
    "nn1.add(Dense(50, activation='sigmoid'))\n",
    "nn1.add(Dense(1))\n",
    "nn1.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#relu neural network\n",
    "nn2 = Sequential(name='sin_wave_relu')\n",
    "nn2.add(Input(1))\n",
    "nn2.add(Dense(30, activation='relu'))\n",
    "nn2.add(Dense(30, activation='relu'))\n",
    "nn2.add(Dense(1))\n",
    "nn2.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#mixed neural network\n",
    "nn3 = Sequential(name='sin_wave_mixed')\n",
    "nn3.add(Input(1))\n",
    "nn3.add(Dense(50, activation='sigmoid'))\n",
    "nn3.add(Dense(50, activation='relu'))\n",
    "nn3.add(Dense(1))\n",
    "nn3.compile(optimizer=Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#train all three neural networks\n",
    "history1 = nn1.fit(x=df['x_scaled'], y=df['y_scaled'],verbose=1, epochs=75)\n",
    "history2 = nn2.fit(x=df['x_scaled'], y=df['y_scaled'],verbose=1, epochs=75)\n",
    "history3 = nn3.fit(x=df['x_scaled'], y=df['y_scaled'],verbose=1, epochs=150)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Check the predictions\n",
    "Before we formulate our trained neural networks in OMLT, we check to see that they adequately represent the data. While we would normally use some accuracy measure, we suffice with a visual plot of the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#note: we calculate the unscaled output for each neural network to check the predictions\n",
    "#nn1\n",
    "y_predict_scaled_sigmoid = nn1.predict(x=df['x_scaled'])\n",
    "y_predict_sigmoid = y_predict_scaled_sigmoid*(std_data['y']) + mean_data['y']\n",
    "\n",
    "#nn2\n",
    "y_predict_scaled_relu = nn2.predict(x=df['x_scaled'])\n",
    "y_predict_relu = y_predict_scaled_relu*(std_data['y']) + mean_data['y']\n",
    "\n",
    "#nn3\n",
    "y_predict_scaled_mixed = nn3.predict(x=df['x_scaled'])\n",
    "y_predict_mixed = y_predict_scaled_mixed*(std_data['y']) + mean_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#create a single plot with the original data and each neural network's predictions\n",
    "fig,ax = plt.subplots(1,figsize = (8,8))\n",
    "ax.plot(x,y,linewidth = 3.0,label = \"data\", alpha = 0.5)\n",
    "ax.plot(x,y_predict_relu,linewidth = 3.0,linestyle=\"dotted\",label = \"relu\")\n",
    "ax.plot(x,y_predict_sigmoid,linewidth = 3.0,linestyle=\"dotted\",label = \"sigmoid\")\n",
    "ax.plot(x,y_predict_mixed,linewidth = 3.0,linestyle=\"dotted\",label = \"mixed\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Formulating Neural Networks with OMLT\n",
    "We now show how OMLT can formulate neural networks within Pyomo. We specifically show how to specify and build different neural network optimization formulations and how to connect them with a broader Pyomo model. In these examples we use Pyomo solvers to find the input that minimizes each neural network output.\n",
    "<br><br>\n",
    "OMLT can formulate what we call full-space and reduced-space neural network representations using the `FullSpaceSmoothNNFormulation` object (for full-space) and `ReducedSpaceSmoothNNFormulation` object (for reduced-space). The reduced-space representation can be represented more compactly than the full-space within an optimization setting (i.e. it produces less variables and constraints), but we will see that full-space representation is necessary to represent non-smooth activation formulations (e.g. ReLU with binary variables).\n",
    "\n",
    "### Reduced Space  (supports smooth activations) <br>\n",
    "The reduced-space representation (`ReducedSpaceSmoothNNFormulation`) provided by OMLT hides intermediate neural network variables and activation functions from the underlying optimizer and represents the neural network using one constraint as following:\n",
    "\n",
    "$\\hat{y} = N(x)$\n",
    "\n",
    "Here, $\\hat{y}$ is a vector of outputs from the neural network, $x$ is a vector of inputs, and $N(\\cdot)$ represents the encoded neural network function that internally uses weights, biases, and activation functions to map $x \\rightarrow \\hat{y}$. From an implementation standpoint, OMLT builds the reduced-space formulation by encoding the sequential layer logic and activation functions as Pyomo `Expression` objects that depend only on the input variables.\n",
    "\n",
    "### Full Space (supports smooth and non-smooth activations) <br>\n",
    "The full space formulation (`FullSpaceSmoothNNFormulation`) creates intermediate variables associated with the neural network nodes and activation functions and exposes them to the optimizer. This is represented by the following set of equations where $x$ and $\\hat{y}$ are again the neural network input and output vectors, and we introduce $\\hat{z}_{\\ell}$ and $z_{\\ell}$ to represent pre-activation and post-activation vectors for each each layer $\\ell$. We further use the notation $\\hat z_{\\ell,i}$ to denote node $i$ in layer $\\ell$ where $N_\\ell$ is the number of nodes in layer $\\ell$ and $N_L$ is the number of layers in the neural network. As such, the first equation maps the input to the first layer values $z_0$, the second equation represents the pre-activation values obtained from the weights, biases, and outputs of the previous layer, the third equation applies the activation function, and the last equation maps the final layer to the output. Note that the reduced-space formulation effectively captures these equations using a single constraint.\n",
    "\n",
    "$\\begin{align*}\n",
    "& x = z_0 &\\\\\n",
    "& \\hat z_{\\ell,i} = \\sum_{j{=}1}^{N_{\\ell-1}} w_{ij} z_j + b_i & \\forall i \\in \\{1,...,N_\\ell \\}, \\quad \\ell \\in \\{1,...N_L\\} \\\\\n",
    "& z_{\\ell,i} = \\sigma(\\hat z_{\\ell}) &  \\forall i \\in \\{1,...,N_\\ell \\}, \\quad \\ell \\in \\{1,...N_L\\} \\\\\n",
    "& \\hat{y} = z_{N_L} &\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### Full Space ReLU with Binary Variables\n",
    "The full space formulation supports non-smooth ReLU activation functions (i.e. the function $z_i = max(0,\\hat{z}_i)$) by using binary indicator variables. When using `ReluBigMFormulation` with a neural network that contains ReLU activations, OMLT will formulate the below set of variables and constraints for each node in a ReLU layer. Here, $q_{\\ell,i}$ is a binary indicator variable that determines whether the output from node $i$ on layer $\\ell$ is $0$ or whether it is $\\hat{z}_{\\ell,i}$. $M_{\\ell,i}^U$ and $M_{\\ell,i}^L$ are 'BigM' constants used to enforce the ReLU logic. Values for 'BigM' are often taken to be arbitrarily large numbers, but OMLT will automatically determine values by propagating the bounds on the input variables.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "& z_{\\ell,i} \\ge \\hat{z}_{\\ell,i} & \\forall i \\in \\{1,...,N_\\ell \\}, \\quad \\ell \\in \\{1,...N_L\\}\\\\\n",
    "& z_{\\ell,i} \\ge 0 & \\forall i \\in \\{1,...,N_\\ell \\}, \\quad \\ell \\in \\{1,...N_L\\}\\\\\n",
    "& z_{\\ell,i} \\le M_{\\ell,i}^L q_{\\ell,i} & \\forall i \\in \\{1,...,N_\\ell \\}, \\quad \\ell \\in \\{1,...N_L\\} \\\\\n",
    "& z_{\\ell,i} \\le \\hat{z}_{\\ell,i} - M_{\\ell,i}^U(1-q_{\\ell,i}) & \\forall i \\in \\{1,...,N_\\ell \\}, \\quad \\ell \\in \\{1,...N_L\\}\n",
    "\\end{align*} \n",
    "$\n",
    "\n",
    "\n",
    "### Full Space ReLU with Complementarity Constraints\n",
    "ReLU activation functions can also be represented using the following complementarity condition:\n",
    "$\n",
    "\\begin{align*}\n",
    "0 \\le (z_{\\ell,i} - \\hat{z}_{\\ell,i}) \\perp z_{\\ell,i} \\ge 0 & \\quad \\forall i \\in \\{1,...,N_\\ell \\}, \\quad \\ell \\in \\{1,...N_L\\}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "This condition means that both of the expressions must be satisfied, where exactly one expression must be satisfied with equality. Hence, we must have that $z_{\\ell,i} \\ge \\hat{z}_{\\ell,i}$ and $z_{\\ell,i} \\ge 0$ with either $z_{\\ell,i} = \\hat{z}_{\\ell,i}$, or $z_{\\ell,i} = 0$.\n",
    "\n",
    "OMLT uses `ReluComplementarityFormulation` to specify that ReLU activation functions should be formulated using complementarity conditions. Within the formulation code, it uses `pyomo.mpec` to transform this complementarity condition into nonlinear constraints which facilitates using smooth optimization solvers (such as Ipopt) to optimize over ReLU activation functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Solving Optimization Problems with Neural Networks using OMLT\n",
    "\n",
    "We now show how to use the above neural network formulations in OMLT for our trained neural networks: `nn1`, `nn2`, and `nn3`. For each formulation we solve the simple optimization problem below using Pyomo where we find the input $x$ that minimizes the output $\\hat y$ of the neural network. \n",
    "\n",
    "$\n",
    "\\begin{align*} \n",
    "& \\min_x \\ \\hat{y}\\\\\n",
    "& s.t. \\hat{y} = N(x) \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "For each neural network we trained, we instantiate a Pyomo `ConcreteModel` and create variables that represent the neural network input $x$ and output $\\hat y$. We also create an objective function that seeks to minimize the output $\\hat y$.\n",
    "\n",
    "Each example uses the same general workflow:\n",
    "- Use the `keras_reader` to import the neural network into a OMLT `NetworkDefinition` object.\n",
    "- Create a Pyomo model with variables `x` and `y` where we intend to minimize `y`.\n",
    "- Create an `OmltBlock`.\n",
    "- Create a formulation object. Note that we use `ReducedSpaceSmoothNNFormulation` for the reduced-space and various full-space formulations for full-space and ReLU. \n",
    "- Build the formulation object on the `OmltBlock`.\n",
    "- Add constraints connecting `x` to the neural network input and `y` to the neural network output.\n",
    "- Solve with an optimization solver (this example uses ipopt).\n",
    "- Query the solution.\n",
    "\n",
    "We also print model size and solution time following each cell where we optimize the Pyomo model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup scaling and input bounds\n",
    "We assume that our Pyomo model operates in the unscaled space with respect to our neural network inputs and outputs. We additionally assume input bounds to our neural networks are given by the limits of our training data. \n",
    "\n",
    "To handle this, OMLT can be given scaling information (in the form of an OMLT scaling object) and input bounds (in the form of a dictionary where indices correspond to neural network indices and values are 2-length tuples of lower and upper bounds). This maintains the space of the optimization problem and scaling is handled by OMLT underneath. The scaling object and input bounds are passed to keras reader method `load_keras_sequential` when importing the associated neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#create an omlt scaling object\n",
    "scaler = omlt.scaling.OffsetScaling(offset_inputs=[mean_data['x']],\n",
    "                    factor_inputs=[std_data['x']],\n",
    "                    offset_outputs=[mean_data['y']],\n",
    "                    factor_outputs=[std_data['y']])\n",
    "\n",
    "#create the input bounds. note that the key `0` corresponds to input `0` and that we also scale the input bounds\n",
    "input_bounds={0:((min(df['x']) - mean_data['x'])/std_data['x'],\n",
    "                 (max(df['x']) - mean_data['x'])/std_data['x'])};\n",
    "print(scaler)\n",
    "print(\"Scaled input bounds: \",input_bounds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network 1: Sigmoid Activations with Full-Space and Reduced-Space Formulations\n",
    "The first neural network contains sigmoid activation functions which we formulate with full-space and reduced-space representations and solve with Ipopt.\n",
    "\n",
    "### Reduced Space Model\n",
    "We begin with the reduced-space formulation and build the Pyomo model according to the above workflow. Note that the reduced-space model only contains 6 variables (`x` and `y` created on the Pyomo model, and the `OmltBlock` scaled and unscaled input and output which get created internally). The full-space formulation (shown next) will contain many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a network definition\n",
    "net_sigmoid = keras_reader.load_keras_sequential(nn1,scaler,input_bounds)\n",
    "\n",
    "#create a pyomo model with variables x and y\n",
    "model1_reduced = pyo.ConcreteModel()\n",
    "model1_reduced.x = pyo.Var(initialize = 0)\n",
    "model1_reduced.y = pyo.Var(initialize = 0)\n",
    "model1_reduced.obj = pyo.Objective(expr=(model1_reduced.y))\n",
    "\n",
    "#create an OmltBlock\n",
    "model1_reduced.nn = OmltBlock()\n",
    "\n",
    "#use the reduced-space formulation\n",
    "formulation1_reduced = ReducedSpaceSmoothNNFormulation(net_sigmoid)\n",
    "model1_reduced.nn.build_formulation(formulation1_reduced)\n",
    "\n",
    "#connect pyomo variables to the neural network\n",
    "@model1_reduced.Constraint()\n",
    "def connect_inputs(mdl):\n",
    "    return mdl.x == mdl.nn.inputs[0]\n",
    "\n",
    "@model1_reduced.Constraint()\n",
    "def connect_outputs(mdl):\n",
    "    return mdl.y == mdl.nn.outputs[0]\n",
    "\n",
    "#solve the model and query the solution\n",
    "status_1_reduced = pyo.SolverFactory('ipopt').solve(model1_reduced, tee=True)\n",
    "solution_1_reduced = (pyo.value(model1_reduced.x),pyo.value(model1_reduced.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print out model size and solution values\n",
    "print(\"Reduced Space Solution:\")\n",
    "print(\"# of variables: \",model1_reduced.nvariables())\n",
    "print(\"# of constraints: \",model1_reduced.nconstraints())\n",
    "print(\"x = \", solution_1_reduced[0])\n",
    "print(\"y = \", solution_1_reduced[1])\n",
    "print(\"Solve Time: \", status_1_reduced['Solver'][0]['Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Full Space Model\n",
    "For the full-space representation we use `NeuralNetworkFormulation` instead of `ReducedSpaceNeuralNetworkFormulation`. The key difference is that this formulation creates additional variables and constraints to represent each node and activation function in the neural network.\n",
    "\n",
    "Note that when we print this model there are over 400 variables and constraints each owing to the number of neural network nodes. The solution consequently takes longer with more iterations (this effect is more pronounced for larger models). The full-space also finds a different local minima, but this was by no means guaranteed to happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net_sigmoid = keras_reader.load_keras_sequential(nn1,scaler,input_bounds)\n",
    "\n",
    "model1_full = pyo.ConcreteModel()\n",
    "model1_full.x = pyo.Var(initialize = 0)\n",
    "model1_full.y = pyo.Var(initialize = 0)\n",
    "model1_full.obj = pyo.Objective(expr=(model1_full.y))\n",
    "model1_full.nn = OmltBlock()\n",
    "\n",
    "formulation2_full = FullSpaceSmoothNNFormulation(net_sigmoid)\n",
    "model1_full.nn.build_formulation(formulation2_full)\n",
    "\n",
    "@model1_full.Constraint()\n",
    "def connect_inputs(mdl):\n",
    "    return mdl.x == mdl.nn.inputs[0]\n",
    "\n",
    "@model1_full.Constraint()\n",
    "def connect_outputs(mdl):\n",
    "    return mdl.y == mdl.nn.outputs[0]\n",
    "\n",
    "status_1_full = pyo.SolverFactory('ipopt').solve(model1_full, tee=True)\n",
    "solution_1_full = (pyo.value(model1_full.x),pyo.value(model1_full.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print out model size and solution values\n",
    "print(\"Full Space Solution:\")\n",
    "print(\"# of variables: \",model1_full.nvariables())\n",
    "print(\"# of constraints: \",model1_full.nconstraints())\n",
    "print(\"x = \", solution_1_full[0])\n",
    "print(\"y = \", solution_1_full[1])\n",
    "print(\"Solve Time: \", status_1_full['Solver'][0]['Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network 2: ReLU Neural Network using Complementarity Constraints and Binary Variables\n",
    "The second neural network contains ReLU activation functions which we can represent using complementarity constraints (which are smooth) or by using different binary variable formulations.\n",
    "\n",
    "### ReLU with Complementarity Constraints\n",
    "To represent ReLU using complementarity constraints we use the `ReluComplementarityFormulation` object. Importantly, the complementarity formulation allows us to solve the optimization problem using a smooth optimizer (in this case Ipopt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net_relu = keras_reader.load_keras_sequential(nn2,scaler,input_bounds)\n",
    "\n",
    "model2_comp = pyo.ConcreteModel()\n",
    "model2_comp.x = pyo.Var(initialize = 0)\n",
    "model2_comp.y = pyo.Var(initialize = 0)\n",
    "model2_comp.obj = pyo.Objective(expr=(model2_comp.y))\n",
    "model2_comp.nn = OmltBlock()\n",
    "\n",
    "formulation2_comp = ReluComplementarityFormulation(net_relu)\n",
    "model2_comp.nn.build_formulation(formulation2_comp)\n",
    "\n",
    "@model2_comp.Constraint()\n",
    "def connect_inputs(mdl):\n",
    "    return mdl.x == mdl.nn.inputs[0]\n",
    "\n",
    "@model2_comp.Constraint()\n",
    "def connect_outputs(mdl):\n",
    "    return mdl.y == mdl.nn.outputs[0]\n",
    "\n",
    "status_2_comp = pyo.SolverFactory('ipopt').solve(model2_comp, tee=True)\n",
    "solution_2_comp = (pyo.value(model2_comp.x),pyo.value(model2_comp.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print out model size and solution values\n",
    "print(\"ReLU Complementarity Solution:\")\n",
    "print(\"# of variables: \",model2_comp.nvariables())\n",
    "print(\"# of constraints: \",model2_comp.nconstraints())\n",
    "print(\"x = \", solution_2_comp[0])\n",
    "print(\"y = \", solution_2_comp[1])\n",
    "print(\"Solve Time: \", status_2_comp['Solver'][0]['Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ReLU with Binary Variables and BigM Constraints\n",
    "For the binary variable formulations of ReLU we can use the `ReluBigMFormulation` object or the `ReluPartitionFormulation` object. The next cell solves the optimization problem using the `ReluBigMFormulation` using Cbc which can handle binary variables. This formulation is also applied automatically if a `NetworkDefinition` contains ReLU activation functions and the user selects the `FullSpaceNNFormulation`. \n",
    "\n",
    "The solution with Cbc tends to take longer than using the `ReluComplementarityFormulation` with Ipopt for this problem, but it is guaranteed to find the global minimum. Also note that the Big-M values are calculated automatically in OMLT using the bounds on the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net_relu = keras_reader.load_keras_sequential(nn2,scaler,input_bounds)\n",
    "\n",
    "model2_bigm = pyo.ConcreteModel()\n",
    "model2_bigm.x = pyo.Var(initialize = 0)\n",
    "model2_bigm.y = pyo.Var(initialize = 0)\n",
    "model2_bigm.obj = pyo.Objective(expr=(model2_bigm.y))\n",
    "model2_bigm.nn = OmltBlock()\n",
    "\n",
    "formulation2_bigm = ReluBigMFormulation(net_relu)\n",
    "model2_bigm.nn.build_formulation(formulation2_bigm)\n",
    "\n",
    "@model2_bigm.Constraint()\n",
    "def connect_inputs(mdl):\n",
    "    return mdl.x == mdl.nn.inputs[0]\n",
    "\n",
    "@model2_bigm.Constraint()\n",
    "def connect_outputs(mdl):\n",
    "    return mdl.y == mdl.nn.outputs[0]\n",
    "\n",
    "status_2_bigm = pyo.SolverFactory('cbc').solve(model2_bigm, tee=False)\n",
    "solution_2_bigm = (pyo.value(model2_bigm.x),pyo.value(model2_bigm.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print out model size and solution values\n",
    "print(\"ReLU BigM Solution:\")\n",
    "print(\"# of variables: \",model2_bigm.nvariables())\n",
    "print(\"# of constraints: \",model2_bigm.nconstraints())\n",
    "print(\"x = \", solution_2_bigm[0])\n",
    "print(\"y = \", solution_2_bigm[1])\n",
    "print(\"Solve Time: \", status_2_bigm['Solver'][0]['Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ReLU Partition Formulation\n",
    "\n",
    "The `ReluPartitionFormulation` generalizes the Big-M formulation by splitting the ReLU logic across subsets of the neural network inputs. This formulation can be used to explore tradeoffs between the size of the optimization model and the tightness of variable bounds. To do this, the `ReluPartitionFormulation` partitions inputs into groups and forms the convex hull over the partitions via disjunctive programming. \n",
    "\n",
    "Since the example neural network in this notebook only contains one input, the partition formulation is equivalent to the Big-M formulation above. The next cell however shows how one might use different partition schemes for a more complex neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net_relu_partition = keras_reader.load_keras_sequential(nn2,scaler,input_bounds)\n",
    "\n",
    "#create a function that partitions a vector of weights w` into `n` partitions\n",
    "#by default, the `ReluPartitionFormulation` will use this function with n=2\n",
    "def partition_split_func(w, n):\n",
    "    sorted_indexes = np.argsort(w)\n",
    "    n = min(n, len(sorted_indexes))\n",
    "    return np.array_split(sorted_indexes, n)\n",
    "\n",
    "#change the number of partitions and create a function we can pass to the formulation\n",
    "#'N = 1' corresponds to BigM, 'N = n_inputs' corresponds to a convex hull formulation\n",
    "N = 1\n",
    "split_func = lambda w: partition_split_func(w, N)\n",
    "\n",
    "model2_partition = pyo.ConcreteModel()\n",
    "model2_partition.x = pyo.Var(initialize = 0)\n",
    "model2_partition.y = pyo.Var(initialize = 0)\n",
    "model2_partition.obj = pyo.Objective(expr=(model2_partition.y))\n",
    "model2_partition.nn = OmltBlock()\n",
    "\n",
    "formulation2_partition = ReluPartitionFormulation(net_relu_partition, split_func=split_func)\n",
    "model2_partition.nn.build_formulation(formulation2_partition)\n",
    "\n",
    "@model2_partition.Constraint()\n",
    "def connect_inputs(mdl):\n",
    "    return mdl.x == mdl.nn.inputs[0]\n",
    "\n",
    "@model2_partition.Constraint()\n",
    "def connect_outputs(mdl):\n",
    "    return mdl.y == mdl.nn.outputs[0]\n",
    "\n",
    "status_2_partition = pyo.SolverFactory('cbc').solve(model2_partition, tee=False)\n",
    "solution_2_partition = (pyo.value(model2_partition.x),pyo.value(model2_partition.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print out model size and solution values\n",
    "print(\"ReLU Partition Solution:\")\n",
    "print(\"# of variables: \",model2_partition.nvariables())\n",
    "print(\"# of constraints: \",model2_partition.nconstraints())\n",
    "print(\"x = \", solution_2_partition[0])\n",
    "print(\"y = \", solution_2_partition[1])\n",
    "print(\"Solve Time: \", status_2_partition['Solver'][0]['Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network 3: Mixed ReLU and Sigmoid Activation Functions\n",
    "The last neural network contains both ReLU and sigmoid activation functions. These networks can be represented by using the complementarity formulation of relu and mixing it with the full-space formulation for the sigmoid functions. This is done using `FullSpaceNNFormulation` and specifying the `ComplementarityReLUActivation` object in an `activation_constraints` dictionary that we pass into the formulation. This dictionary tells the formulation object to override how it formulates \"relu\" activation constraints (which are BigM by default). Here, we tell it to use the complementarity formulation for any layer that contains the \"relu\" activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net_mixed = keras_reader.load_keras_sequential(nn3,scaler,input_bounds)\n",
    "\n",
    "model3_mixed = pyo.ConcreteModel()\n",
    "model3_mixed.x = pyo.Var(initialize = 0)\n",
    "model3_mixed.y = pyo.Var(initialize = 0)\n",
    "model3_mixed.obj = pyo.Objective(expr=(model3_mixed.y))\n",
    "model3_mixed.nn = OmltBlock()\n",
    "\n",
    "formulation3_mixed = FullSpaceNNFormulation(net_mixed,activation_constraints={\n",
    "            \"relu\": ComplementarityReLUActivation()})\n",
    "model3_mixed.nn.build_formulation(formulation3_mixed)\n",
    "\n",
    "@model3_mixed.Constraint()\n",
    "def connect_inputs(mdl):\n",
    "    return mdl.x == mdl.nn.inputs[0]\n",
    "\n",
    "@model3_mixed.Constraint()\n",
    "def connect_outputs(mdl):\n",
    "    return mdl.y == mdl.nn.outputs[0]\n",
    "\n",
    "status_3_mixed = pyo.SolverFactory('ipopt').solve(model3_mixed, tee=True)\n",
    "solution_3_mixed = (pyo.value(model3_mixed.x),pyo.value(model3_mixed.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print out model size and solution values\n",
    "print(\"Mixed NN Solution:\")\n",
    "print(\"# of variables: \",model3_mixed.nvariables())\n",
    "print(\"# of constraints: \",model3_mixed.nconstraints())\n",
    "print(\"x = \", solution_3_mixed[0])\n",
    "print(\"y = \", solution_3_mixed[1])\n",
    "print(\"Solve Time: \", status_3_mixed['Solver'][0]['Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Final Plots and Discussion\n",
    "\n",
    "We lastly plot the results of each optimization problem. Some of the main take-aways from this notebook are as follows:\n",
    "- A broad set of dense neural network architectures can be represented in Pyomo using OMLT. This notebook used the Keras reader to import sequential Keras models but OMLT also supports using ONNX models (see `import_network.ipynb`). OMLT additionally supports Convolutional Neural Networks (see `mnist_example_cnn.ipynb`).\n",
    "- The reduced-space formulation provides a computationally tractable means to represent neural networks that contain smooth activation functions and can be used with continuous optimizers to obtain local solutions.\n",
    "- The full-space formulation permits representing ReLU activation functions using either complementarity or 'BigM' approaches with binary variables (as well as partition-based approaches).\n",
    "- The full-space formulation further allows one to optimize over neural networks that contain mixed activation functions by formulating ReLU logic as complementarity conditions.\n",
    "- Using binary variables to represent ReLU (with Big-M) allows the use of global MIP solvers (if the rest of the optimization problem permits), whereas the complementarity formulation permits the use of local solvers that tend to be more scalable (if there are no binary/integer variables).\n",
    "- The partition formulation can normally be used to improve upon the Big-M formulation, but in this notebook they are equivalent because there is only one input in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#create a plot with 3 subplots\n",
    "fig,axs = plt.subplots(1,3,figsize = (24,8))\n",
    "\n",
    "#nn1 - sigmoid\n",
    "axs[0].plot(x,y_predict_sigmoid,linewidth = 3.0,linestyle=\"dotted\",color = \"orange\")\n",
    "axs[0].set_title(\"sigmoid\")\n",
    "axs[0].scatter([solution_1_reduced[0]],[solution_1_reduced[1]],color = \"black\",s = 300, label=\"reduced space\")\n",
    "axs[0].scatter([solution_1_full[0]],[solution_1_full[1]],color = \"blue\",s = 300, label=\"full space\")\n",
    "axs[0].legend()\n",
    "\n",
    "#nn2 - relu\n",
    "axs[1].plot(x,y_predict_relu,linewidth = 3.0,linestyle=\"dotted\",color = \"green\")\n",
    "axs[1].set_title(\"relu\")\n",
    "axs[1].scatter([solution_2_comp[0]],[solution_2_comp[1]],color = \"black\",s = 300, label=\"complementarity\")\n",
    "axs[1].scatter([solution_2_bigm[0]],[solution_2_bigm[1]],color = \"blue\",s = 300, label=\"bigm\")\n",
    "axs[1].scatter([solution_2_partition[0]],[solution_2_partition[1]],color = \"purple\",s = 300, label=\"partition\")\n",
    "axs[1].legend()\n",
    "\n",
    "#nn3 - mixed\n",
    "axs[2].plot(x,y_predict_mixed,linewidth = 3.0,linestyle=\"dotted\", color = \"red\")\n",
    "axs[2].set_title(\"mixed\")\n",
    "axs[2].scatter([solution_3_mixed[0]],[solution_3_mixed[1]],color = \"black\",s = 300);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
