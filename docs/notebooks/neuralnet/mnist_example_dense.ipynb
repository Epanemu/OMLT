{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Optimal adversaries for dense MNIST model\n",
    "\n",
    "This notebook gives an example where OMLT is used to find adversarial examples for a trained dense neural network. We follow the below steps:<br>\n",
    "1.) A neural network with ReLU activation functions is trained to classify images from the MNIST dataset <br>\n",
    "2.) OMLT is used to generate a mixed-integer encoding of the trained model using the big-M formulation <br>\n",
    "3.) The model is optimized to find the maximum classification error (defined by an \"adversarial\" label) over a small input region <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Setup\n",
    "This notebook assumes you have a working PyTorch environment to train the neural network for classification. The neural network is then formulated in Pyomo using OMLT which therefore requires working Pyomo and OMLT installations.\n",
    "\n",
    "The required Python libraries used this notebook are as follows: <br>\n",
    "- `numpy`: used for manipulate input data <br>\n",
    "- `torch`: the machine learning language we use to train our neural network\n",
    "- `torchvision`: a package containing the MNIST dataset\n",
    "- `pyomo`: the algebraic modeling language for Python, it is used to define the optimization model passed to the solver\n",
    "- `omlt`: the package this notebook demonstates. OMLT can formulate machine learning models (such as neural networks) within Pyomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import requisite packages\n",
    "#data manipulation\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "#pytorch for training neural network\n",
    "import torch, torch.onnx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "#pyomo for optimization\n",
    "import pyomo.environ as pyo\n",
    "\n",
    "#omlt for interfacing our neural network with pyomo\n",
    "from omlt import OmltBlock\n",
    "from omlt.neuralnet import NeuralNetworkFormulation\n",
    "from omlt.io.onnx import write_onnx_model_with_bounds, load_onnx_neural_network_with_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data and Train a Neural Network\n",
    "\n",
    "We begin by loading the MNIST dataset as `DataLoader` objects with pre-set training and testing batch sizes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training and test batch sizes\n",
    "train_kwargs = {'batch_size': 64}\n",
    "test_kwargs = {'batch_size': 1000}\n",
    "\n",
    "#build DataLoaders for training and test sets\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "dataset2 = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the structure of the dense neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #define layers of neural network\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1  = nn.Linear(784, hidden_size)\n",
    "        self.hidden2  = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output  = nn.Linear(hidden_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define forward pass of neural network\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define simple functions for training and testing the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function computes loss and its gradient on batch, and prints status after every 200 batches\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train(); criterion = nn.NLLLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.view(-1, 28*28))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "#testing function computes loss and prints overall model accuracy on test set\n",
    "def test(model, test_loader):\n",
    "    model.eval(); criterion = nn.NLLLoss( reduction='sum')\n",
    "    test_loss = 0; correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data.view(-1, 28*28))\n",
    "            test_loss += criterion(output, target).item()  \n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the neural network on the dataset.\n",
    "Training here is performed using the `Adadelta` optimizer for five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.308089\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.351651\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.291912\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.252940\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.070289\n",
      "\n",
      "Test set: Average loss: 0.1741, Accuracy: 9455/10000 (95%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.395864\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.099001\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.141914\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.034890\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.138411\n",
      "\n",
      "Test set: Average loss: 0.1219, Accuracy: 9613/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.039525\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.182685\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.066296\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.037592\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.160053\n",
      "\n",
      "Test set: Average loss: 0.1093, Accuracy: 9657/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.024848\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.029397\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.076984\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.037213\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.052514\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9712/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.027488\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.089607\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.018610\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.015982\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.058869\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9718/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define model and optimizer\n",
    "model = Net()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "#train neural network for five epochs\n",
    "for epoch in range(5):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a MIP Formulation for the Trained Neural Network\n",
    "\n",
    "We are now ready to use OMLT to formulate the trained model within a Pyomo optimization model. The nonsmooth ReLU activation function requires using a full-space representation, which uses the `NeuralNetworkFormulation` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a neural network without the final `LogSoftmax` activation. Although this activation helps greatly in training the neural network model, it is not trivial to encode in the optimization model. The ranking of the output labels remains the same without the activation, so it can be omitted when finding optimal adversaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NoSoftmaxNet(nn.Module):\n",
    "    #define layers of neural network\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1  = nn.Linear(784, hidden_size)\n",
    "        self.hidden2  = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output  = nn.Linear(hidden_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    #define forward pass of neural network\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "#create neural network without LogSoftmax and load parameters from existing model\n",
    "model2 = NoSoftmaxNet()\n",
    "model2.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an instance of the optimal adversary problem. We formulate the optimization problem as: <br>\n",
    "\n",
    "$\n",
    "\\begin{align*} \n",
    "& \\max_x \\ y_k - y_j \\\\\n",
    "& s.t. y_k = N_k(x) \\\\ \n",
    "&\\quad |x - \\bar{x}|_\\infty \\leq 0.05\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $\\bar{x}$ corresponds to an image in the test dataset with true label `j`, and $N_k(x)$ is the value of the neural network output corresponding to adversarial label `k` given input `x`. PyTorch needs to trace the model execution to export it to ONNX, so we also define a dummy input tensor `x_temp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image and true label from test set with index 'problem_index'\n",
    "problem_index = 0\n",
    "image = dataset2[problem_index][0].view(-1,28*28).detach().numpy()\n",
    "label = dataset2[problem_index][1]\n",
    "\n",
    "#define input region defined by infinity norm\n",
    "epsilon_infty = 5e-2\n",
    "lb = np.maximum(0, image - epsilon_infty)\n",
    "ub = np.minimum(1, image + epsilon_infty)\n",
    "\n",
    "#save input bounds as dictionary\n",
    "input_bounds = {}\n",
    "for i in range(28*28):\n",
    "    input_bounds[i] = (float(lb[0][i]), float(ub[0][i])) \n",
    "    \n",
    "#define dummy input tensor    \n",
    "x_temp = dataset2[problem_index][0].view(-1,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now export the PyTorch model as an ONNX model and use `load_onnx_neural_network_with_bounds` to load it into OMLT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as f:\n",
    "    #export neural network to ONNX\n",
    "    torch.onnx.export(\n",
    "        model2,\n",
    "        x_temp,\n",
    "        f,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    #write ONNX model and its bounds using OMLT\n",
    "    write_onnx_model_with_bounds(f.name, None, input_bounds)\n",
    "    #load the network definition from the ONNX model\n",
    "    network_definition = load_onnx_neural_network_with_bounds(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check before creating the optimization model, we can print the properties of the neural network layers from `network_definition`. This allows us to check input/output sizes, as well as activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tInputLayer(input_size=[784], output_size=[784])\tlinear\n",
      "1\tDenseLayer(input_size=[784], output_size=[50])\trelu\n",
      "2\tDenseLayer(input_size=[50], output_size=[50])\trelu\n",
      "3\tDenseLayer(input_size=[50], output_size=[10])\tlinear\n"
     ]
    }
   ],
   "source": [
    "for layer_id, layer in enumerate(network_definition.layers):\n",
    "    print(f\"{layer_id}\\t{layer}\\t{layer.activation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can load `network_definition` as a full-space `NeuralNetworkFormulation` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "formulation = NeuralNetworkFormulation(network_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Optimal Adversary Problem in Pyomo\n",
    "\n",
    "We now encode the trained neural network in a Pyomo model from the `NeuralNetworkFormulation` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pyomo model\n",
    "m = pyo.ConcreteModel()\n",
    "\n",
    "#create an OMLT block for the neural network and build its formulation\n",
    "m.nn = OmltBlock()\n",
    "m.nn.build_formulation(formulation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an adversarial label as the true label plus one (or zero if the true label is nine), as well as the objective function for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = (label + 1) % 10\n",
    "m.obj = pyo.Objective(expr=(-(m.nn.outputs[adversary]-m.nn.outputs[label])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we solve the optimal adversary problem using a mixed-integer solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2023-01-12\n",
      "Using license file /Users/calvintsay/gurobi.lic\n",
      "Read LP format model from file /var/folders/pc/7mzx4b956_lb2l8_ryngwydc0000gn/T/tmpdznvauby.pyomo.lp\n",
      "Reading time = 0.05 seconds\n",
      "x2693: 2109 rows, 2693 columns, 46307 nonzeros\n",
      "Gurobi Optimizer version 9.1.1 build v9.1.1rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 2109 rows, 2693 columns and 46307 nonzeros\n",
      "Model fingerprint: 0xcb2bfc90\n",
      "Variable types: 2593 continuous, 100 integer (100 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [4e-07, 1e+01]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [5e-03, 6e+01]\n",
      "  RHS range        [1e-03, 1e+01]\n",
      "Presolve removed 1802 rows and 1675 columns\n",
      "Presolve time: 0.13s\n",
      "Presolved: 307 rows, 1018 columns, 32312 nonzeros\n",
      "Variable types: 945 continuous, 73 integer (73 binary)\n",
      "\n",
      "Root relaxation: objective -7.081046e+00, 170 iterations, 0.01 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   -7.08105    0   37          -   -7.08105      -     -    0s\n",
      "     0     0   -4.26410    0   39          -   -4.26410      -     -    0s\n",
      "     0     0   -2.58503    0   38          -   -2.58503      -     -    0s\n",
      "     0     0   -2.57364    0   39          -   -2.57364      -     -    0s\n",
      "     0     0   -2.57050    0   39          -   -2.57050      -     -    0s\n",
      "     0     0   -1.39704    0   39          -   -1.39704      -     -    0s\n",
      "     0     0   -1.17117    0   39          -   -1.17117      -     -    0s\n",
      "     0     0   -1.15873    0   39          -   -1.15873      -     -    0s\n",
      "     0     0   -1.15466    0   39          -   -1.15466      -     -    0s\n",
      "H    0     0                       3.4338207   -1.15466   134%     -    0s\n",
      "     0     0   -0.70404    0   37    3.43382   -0.70404   121%     -    0s\n",
      "     0     0   -0.54411    0   37    3.43382   -0.54411   116%     -    0s\n",
      "     0     0   -0.52424    0   37    3.43382   -0.52424   115%     -    0s\n",
      "     0     0   -0.52405    0   37    3.43382   -0.52405   115%     -    0s\n",
      "H    0     0                       3.2546376   -0.40898   113%     -    0s\n",
      "     0     0    0.05395    0   34    3.25464    0.05395  98.3%     -    0s\n",
      "     0     0    0.51005    0   34    3.25464    0.51005  84.3%     -    0s\n",
      "     0     0    0.53144    0   34    3.25464    0.53144  83.7%     -    0s\n",
      "     0     0    0.79751    0   34    3.25464    0.79751  75.5%     -    0s\n",
      "     0     0    1.45216    0   25    3.25464    1.45216  55.4%     -    1s\n",
      "     0     0    1.45216    0   25    3.25464    1.45216  55.4%     -    1s\n",
      "     0     2    1.45216    0   25    3.25464    1.45216  55.4%     -    1s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 1\n",
      "  Implied bound: 16\n",
      "  MIR: 34\n",
      "  Flow cover: 1\n",
      "  RLT: 38\n",
      "  Relax-and-lift: 1\n",
      "\n",
      "Explored 52 nodes (1737 simplex iterations) in 1.25 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 3.25464 3.43382 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.254637600541e+00, best bound 3.254637600541e+00, gap 0.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Problem': [{'Name': 'x2693', 'Lower bound': 3.2546376005408444, 'Upper bound': 3.2546376005408444, 'Number of objectives': 1, 'Number of constraints': 2109, 'Number of variables': 2693, 'Number of binary variables': 100, 'Number of integer variables': 100, 'Number of continuous variables': 2593, 'Number of nonzeros': 46307, 'Sense': 'minimize'}], 'Solver': [{'Status': 'ok', 'Return code': '0', 'Message': 'Model was solved to optimality (subject to tolerances), and an optimal solution is available.', 'Termination condition': 'optimal', 'Termination message': 'Model was solved to optimality (subject to tolerances), and an optimal solution is available.', 'Wall time': '1.2540309429168701', 'Error rc': 0, 'Time': 1.4695830345153809}], 'Solution': [OrderedDict([('number of solutions', 0), ('number of solutions displayed', 0)])]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyo.SolverFactory('gurobi').solve(m, tee=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8f31a8284ce774e9ad8d309790c576c984c0620550967f9ef361ac8e66f487d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
