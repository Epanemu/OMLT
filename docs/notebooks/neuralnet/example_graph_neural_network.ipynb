{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Optimizing over trained graph neural networks\n",
    "\n",
    "This notebook gives examples where OMLT is used to optimize over trained graph neural networks (GNNs). We follow the below steps:\n",
    "\n",
    "1.) A general definition of GNNs is provided. For any GNN that fits our definition, it could be transformed into a Dense NN and then exported into OMLT.\n",
    "\n",
    "2.) We give an example to show how to transform a GNN into a Dense NN. For simplicity, we skip the training process and just use random parameters.\n",
    "\n",
    "3.) OMLT is used to generate a mixed-interger encoding of the trained GNN. \n",
    "\n",
    "4.) We consider two cases: one has fixed graph structure, another one has non-fixed graph structure. For each case, the output of the GNN is minimized.\n",
    "\n",
    "\n",
    "## Library Setup\n",
    "\n",
    "This notebook assumes you have a working PyTorch environment to define a Dense NN. This Dense NN is then formulated in Pyomo using OMLT which therefore requires working Pyomo and OMLT installations.\n",
    "\n",
    "The required Python libraries used in this notebook are as follows:\n",
    "\n",
    "- `numpy`: used for transformation of parameters\n",
    "\n",
    "- `torch`: the machine learning language we use to define our Dense NN\n",
    "\n",
    "- `pyomo`: the algebraic modeling language for Python, it is used to define the optimization model passed to the solver\n",
    "\n",
    "- `onnx`: used to express trained neural network models\n",
    "\n",
    "- `omlt`: the package this notebook demonstates. OMLT can formulate machine learning (such as neural networks) within Pyomo\n",
    "\n",
    "**NOTE:** This notebbook alse assumes you have a working MIP solver executable to solve optimization problems in Pyomo. The open-source solver CBC is called by default. \n",
    "\n",
    "\n",
    "## Definition of GNNs\n",
    "\n",
    "We define a GNN with $L$ layers as follows:\n",
    "\n",
    "   \\begin{equation*}\n",
    "\t\t\\begin{aligned}\n",
    "\t\t\tGNN:\\underbrace{\\mathbb R^{d_0}\\otimes\\cdots\\otimes\\mathbb R^{d_0}}_{n \\rm{times}}\\to\\underbrace{\\mathbb R^{d_L}\\otimes\\cdots\\otimes\\mathbb R^{d_L}}_{n\\ \\rm{times}}\n",
    "\t\t\\end{aligned}\n",
    "\t\\end{equation*}\n",
    "    \n",
    "where $V$ is the set of nodes of the input graph, $n=|V|$ is the number of nodes. \n",
    "\n",
    "Let $\\mathbf{x}_v^{(0)} \\in \\mathbb{R}^{d_0}$ be the input features for node $v$. Then, the $l$-th layer ($l=1,2,\\dots,L$) is defined by:\n",
    "\t\\begin{equation*}\n",
    "\t\t\\begin{aligned}\n",
    "\t\t\t\\mathbf{x}_v^{(l)}=\\sigma\\left(\\sum\\limits_{u\\in\\mathcal N(v)\\cup\\{v\\}}\\mathbf{w}_{u\\to v}^{(l)}\\mathbf{x}_u^{(l-1)}+\\mathbf{b}_{v}^{(l)}\\right),~\\forall v\\in V\n",
    "\t\t\\end{aligned}\n",
    "\t\\end{equation*}\n",
    "where $\\mathcal N(v)$ is the set of all neighbors of $v$, $\\sigma$ could be identity or any activation function.\n",
    "\n",
    "*Dimensionality:* $\\mathbf{x}_u^{(l-1)}\\in\\mathbb R^{d_{l-1}}, \\mathbf{x}_v^{(l)},\\mathbf{b}_v^{(l)}\\in\\mathbb R^{d_l}, \\mathbf{w}_{u\\to v}^{(l)}\\in\\mathbb R^{d_l}\\times \\mathbb R^{d_{l-1}}$.\n",
    "\n",
    "Stack $\\{\\mathbf{x}_v^{(l)}\\}_{v\\in V}$ as a vector $\\mathbf{X}^{(l)}\\in \\mathbb R^{nd_l}$. Rewrite previous definition as:\n",
    "    \\begin{equation*}\n",
    "        \\begin{aligned}\n",
    "            \\mathbf{X}^{(l)}=\\sigma\\left(\\mathbf{W}^{(l)}\\mathbf{X}^{(l-1)}+\\mathbf{B}^{(l)}\\right)\n",
    "        \\end{aligned}\n",
    "    \\end{equation*}\n",
    "where $\\mathbf{W}^{(l)}\\in\\mathbb R^{nd_[\\times nd_{l-1}}$ is a sparse matrix with nonzero sub-matrices $\\{\\mathbf{w}_{u\\to v}^{(l)}\\}_{v\\in V,u\\in\\mathcal N(v)\\cup\\{v\\}}$ and $\\mathbf{B}^{(l)}\\in\\mathbb R^{nd_l}$ is the stack of $\\{\\mathbf{b}_v^{(l)}\\}_{v\\in V}$.\n",
    "\n",
    "If the input graph structure is fixed, then weights ($\\mathbf{w}_{u\\to v}^{(l)}$), bias ($\\mathbf{b}_{v}^{(l)}$), and links between layers (determined by $\\mathcal N(v)$) are all fixed after the GNN is trained. In this case, the second definition is equivalent to a dense layer. It suffices to define a Dense NN with weights $\\mathbf{W}^{(l)}$ and bias $\\mathbf{B}^{(l)}$. \n",
    "\n",
    "\n",
    "## Formulating Trained GNNs with OMLT: Fixed Graph Structure\n",
    "\n",
    "\n",
    "### Import Requisite Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters manipulation\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "# pytorch for defining Dense NN\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# pyomo for optimization\n",
    "import pyomo.environ as pyo\n",
    "\n",
    "# omlt for interfacing our neural network with pyomo\n",
    "from omlt import OmltBlock\n",
    "from omlt.neuralnet import ReluBigMFormulation\n",
    "from omlt.neuralnet.network_definition import gnn_layer_definition\n",
    "from omlt.io.onnx import write_onnx_model_with_bounds, load_onnx_neural_network_with_bounds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrcut a GNN with Random Parameters\n",
    "\n",
    "We use a simple GNN as an example, which consists of a GraphSAGE layer, an add pooling layer, and a dense layer with single output. Let the input and output features of the GraphSAGE layer are 2 and 3, respectively. \n",
    "\n",
    "The GraphSAGE layer is defined by:\n",
    "    \\begin{equation*}\n",
    "       \\mathbf{x}_v^{(l)}=\\sigma\\left(\\mathbf{w_1}^{(l)}\\mathbf{x}_v^{(l-1)}+\\mathbf{w_2}^{(l)}\\sum\\limits_{u\\in\\mathcal N(v)}\\mathbf{x}_u^{(l-1)}+\\mathbf{b}^{(l)}\\right)\n",
    "    \\end{equation*}\n",
    "where a sum aggregation is used.\n",
    "\n",
    "For the fixed graph structure, assume that it is a line graph with $N=3$ nodes, i.e., the adjacency matrix $A=\\begin{pmatrix}1 & 1 & 0\\\\1 & 1 & 1\\\\ 0 & 1 & 1\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph structure\n",
    "# number of nodes\n",
    "N = 3\n",
    "# adjacency matrix\n",
    "A = np.array([[1,1,0],[1,1,1],[0,1,1]])\n",
    "\n",
    "# in/out features\n",
    "in_features = 2\n",
    "out_features = 3\n",
    "\n",
    "# architecture of GNN\n",
    "# sage: in_features to out_features for each node, with ReLU as activation\n",
    "# add_pool: read out, sum out_features of each node\n",
    "# dense: out_features to 1\n",
    "gnn_layers = ['sage', 'add_pool', 'dense']\n",
    "activations = [True, False, False]\n",
    "\n",
    "# randomly generate GNN parameters from (-1,1)\n",
    "# in practice, these paprameters should be extracted from the trained GNN\n",
    "np.random.seed(123)\n",
    "sage_w1 = 2.* np.random.rand(out_features, in_features) -1.\n",
    "sage_w2 = 2.* np.random.rand(out_features, in_features) -1.\n",
    "sage_b = 2. * np.random.rand(out_features) - 1.\n",
    "\n",
    "dense_w = 2.* np.random.rand(1, out_features) - 1.\n",
    "dense_b = 2.* np.random.rand(1) - 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming a GNN into a Dense NN\n",
    "\n",
    "The GraphSAGE layer could be rewritten as a dense layer with parameters:\n",
    "\n",
    "   \\begin{equation*}\n",
    "        \\mathbf{W}=\\begin{pmatrix}\n",
    "            \\mathbf{w_1} & \\mathbf{w_2} & \\mathbf{0} \\\\\n",
    "            \\mathbf{w_2} & \\mathbf{w_1} & \\mathbf{w_2} \\\\\n",
    "            \\mathbf{0} & \\mathbf{w_2} & \\mathbf{w_1} \\\\\n",
    "        \\end{pmatrix},\n",
    "        \\mathbf{B}=\\begin{pmatrix}\n",
    "        \\mathbf{b}\\\\\\mathbf{b}\\\\\\mathbf{b}\n",
    "        \\end{pmatrix}\n",
    "    \\end{equation*}\n",
    "    \n",
    "It is straightforward to rewritte the add pooling layer into a dense layer. See the following code for details.\n",
    "\n",
    "See below as a mapping between a GNN and a Dense NN with format \"layer (in_channel, out_channel)\":\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\text{GraphSAGE(2, 3)}   &\\Rightarrow \\text{dense(6, 9)}\\\\\n",
    "        \\text{add pooling(9, 3)}  &\\Rightarrow \\text{dense(9, 3)}\\\\\n",
    "        \\text{dense(3, 1)}      &\\Rightarrow \\text{dense(3, 1)}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchModel(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(\n",
      "      in_features=6, out_features=9, bias=True\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): Linear(in_features=9, out_features=3, bias=True)\n",
      "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# transform a sage layer to dense layer\n",
    "# N is the number of nodes\n",
    "# w1,w2,b are parameters in a sage layer\n",
    "def SAGE_to_Dense(N, A, w1, w2, b):\n",
    "    out_channel, in_channel = w1.shape\n",
    "    weight = np.zeros((N*out_channel, N*in_channel))\n",
    "    bias = np.zeros(N*out_channel)\n",
    "    for u in range(N):\n",
    "        for v in range(N):\n",
    "            if u == v:\n",
    "                weight[u*out_channel:(u+1)*out_channel, v*in_channel:(v+1)*in_channel] = w2\n",
    "            else:\n",
    "                weight[u*out_channel:(u+1)*out_channel, v*in_channel:(v+1)*in_channel] = w1 * A[u,v]\n",
    "        bias[u*out_channel:(u+1)*out_channel] = b\n",
    "    return weight, bias\n",
    "\n",
    "params = []\n",
    "channels = []\n",
    "channels.append(N * in_features)\n",
    "\n",
    "for layer in gnn_layers:\n",
    "    if layer == 'sage':\n",
    "        params.append(SAGE_to_Dense(N,A,sage_w1,sage_w2,sage_b))\n",
    "        channels.append(sage_w1.shape[0] * N)\n",
    "    elif layer == 'dense':\n",
    "        params.append((dense_w,dense_b))\n",
    "        channels.append(w.shape[0])\n",
    "    elif layer == 'add_pool':\n",
    "        channels.append(channels[-1] // N)\n",
    "        w = np.zeros((channels[-1],channels[-2]))\n",
    "        for i in range(channels[-1]):\n",
    "            for j in range(N):\n",
    "                w[i, i+j*channels[-1]] = 1.\n",
    "        b = np.zeros(channels[-1])\n",
    "        params.append((w,b))\n",
    "\n",
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self, L, params, activations):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for l in range(L):\n",
    "            layers.append(nn.Linear(params[l][0].shape[1], params[l][0].shape[0]))\n",
    "            layers[-1].weight = nn.Parameter(torch.tensor(params[l][0], dtype=torch.float64))\n",
    "            layers[-1].bias = nn.Parameter(torch.tensor(params[l][1], dtype=torch.float64))\n",
    "            if activations[l]:\n",
    "                layers[-1].relu = nn.ReLU()\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "model_dense = PyTorchModel(len(channels)-1, params, activations)\n",
    "print(model_dense)\n",
    "# for param in model_dense.parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a MIP Formulation and Solve the Optimization Problem\n",
    "\n",
    "We can now export the PyTorch model as an ONNX model and use `load_onnx_neural_network_with_bounds` to load it into OMLT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros(channels[0], dtype=torch.float64)\n",
    "dummy_input.requires_grad=True\n",
    "input_bounds = [(-1., 1.) for _ in range(channels[0])]\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as f:\n",
    "    #export neural network to ONNX\n",
    "    torch.onnx.export(\n",
    "        model_dense,\n",
    "        dummy_input,\n",
    "        f,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "    )\n",
    "    #write ONNX model and its bounds using OMLT\n",
    "    write_onnx_model_with_bounds(f.name, None, input_bounds)\n",
    "    #load the network definition from the ONNX model\n",
    "    network_definition = load_onnx_neural_network_with_bounds(f.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check before creating the optimization model, we can print the properties of the neural network layers from `network_definition`. This allows us to check input/output sizes, as well as activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tInputLayer(input_size=[6], output_size=[6])\tlinear\n",
      "1\tDenseLayer(input_size=[6], output_size=[9])\tlinear\n",
      "2\tDenseLayer(input_size=[9], output_size=[3])\tlinear\n",
      "3\tDenseLayer(input_size=[3], output_size=[1])\tlinear\n"
     ]
    }
   ],
   "source": [
    "for layer_id, layer in enumerate(network_definition.layers):\n",
    "    print(f\"{layer_id}\\t{layer}\\t{layer.activation}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can load `network_definition` as a full-space `ReluBigMFormulation` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "formulation = ReluBigMFormulation(network_definition)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now encode the Dense NN in a Pyomo model from the `ReluBigMFormulation` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pyomo model\n",
    "m = pyo.ConcreteModel()\n",
    "\n",
    "# create an OMLT block for the neural network and build its formulation\n",
    "m.nn = OmltBlock()\n",
    "m.nn.build_formulation(formulation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function as the single output of the Dense NN and solve the minimization problem using a mixed integer solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.5 \n",
      "Build Date: Dec  8 2020 \n",
      "\n",
      "command line - /rds/general/user/sz421/home/anaconda3/envs/OMLT/bin/cbc -printingOptions all -import /var/tmp/pbs.7796016.pbs/tmpazbklmrp.pyomo.lp -stat=1 -solve -solu /var/tmp/pbs.7796016.pbs/tmpazbklmrp.pyomo.soln (default strategy 1)\n",
      "Option for printingOptions changed from normal to all\n",
      "Presolve 1 (-39) rows, 7 (-39) columns and 7 (-114) elements\n",
      "Statistics for presolved model\n",
      "\n",
      "\n",
      "Problem has 1 rows, 7 columns (7 with objective) and 7 elements\n",
      "There are 7 singletons with objective \n",
      "Column breakdown:\n",
      "0 of type 0.0->inf, 0 of type 0.0->up, 0 of type lo->inf, \n",
      "7 of type lo->up, 0 of type free, 0 of type fixed, \n",
      "0 of type -inf->0.0, 0 of type -inf->up, 0 of type 0.0->1.0 \n",
      "Row breakdown:\n",
      "0 of type E 0.0, 0 of type E 1.0, 0 of type E -1.0, \n",
      "1 of type E other, 0 of type G 0.0, 0 of type G 1.0, \n",
      "0 of type G other, 0 of type L 0.0, 0 of type L 1.0, \n",
      "0 of type L other, 0 of type Range 0.0->1.0, 0 of type Range other, \n",
      "0 of type Free \n",
      "Presolve 1 (-39) rows, 7 (-39) columns and 7 (-114) elements\n",
      "0  Obj 0.34521997 Primal inf 1.2424353 (1) Dual inf 3.9956189 (4)\n",
      "1  Obj -1.7190541\n",
      "Optimal - objective value -1.7190541\n",
      "After Postsolve, objective -1.7190541, infeasibilities - dual 0 (0), primal 0 (0)\n",
      "Optimal objective -1.719054146 - 1 iterations time 0.002, Presolve 0.00\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Problem': [{'Name': 'unknown', 'Lower bound': -1.719054146, 'Upper bound': -1.719054146, 'Number of objectives': 1, 'Number of constraints': 40, 'Number of variables': 46, 'Number of nonzeros': 7, 'Sense': 'minimize'}], 'Solver': [{'Status': 'ok', 'User time': -1.0, 'System time': 0.0, 'Wallclock time': 0.0, 'Termination condition': 'optimal', 'Termination message': 'Model was solved to optimality (subject to tolerances), and an optimal solution is available.', 'Statistics': {'Branch and bound': {'Number of bounded subproblems': None, 'Number of created subproblems': None}, 'Black box': {'Number of iterations': 1}}, 'Error rc': 0, 'Time': 0.033590078353881836}], 'Solution': [OrderedDict([('number of solutions', 0), ('number of solutions displayed', 0)])]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.obj = pyo.Objective(expr=(m.nn.outputs[0]))\n",
    "pyo.SolverFactory('cbc').solve(m, tee=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating Trained GNNs with OMLT: Non-fixed Graph Structure\n",
    "\n",
    "When the input graph structure is not fixed, elements in the adjacency matrix $A$ are decision variables. In this case, $\\mathcal N(v)$ is not given anymore. Additionally, $\\mathbf{w}_{u\\to v}^{(l)},\\mathbf{b}_v^{(l)}$ may contain the graph information, which makes them be variables.\n",
    "\n",
    "Assume that $\\mathbf{w}_{u\\to v}^{(l)},\\mathbf{b}_v^{(l)}$ are fixed. Then we can derive a big-M formulation to handle GNN layers with non-fixed graph structure.\n",
    "\n",
    "Observe that the existence of edge $u\\to v$ determines the contribution link from $\\mathbf{x}_u^{(l-1)}$ to $\\mathbf{x}_v^{(l)}$. Adding binary variables $A_{u,v}$ for all $u,v\\in V$, we can formulate GNNs in a bilinear way:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{x}_v^{(l)}=\\sigma\\left(\\sum\\limits_{u\\in V}A_{u,v}\\mathbf{w}_{u\\to v}^{(l)}\\mathbf{x}_u^{(l-1)}+\\mathbf{b}_{v}^{(l)}\\right), \\forall v\\in V\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "This bilinear formulation involves quadratic constraints. Instead of using binary variables to directly control the existence of contributions between nodes, we introduce auxiliary variables $\\mathbf{\\bar x}_{u\\to v}^{(l-1)}$ to represent the contribution from node $u$ to node $v$ in $l$th layer:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{x}_v^{(l)}=\\sigma\\left(\\sum\\limits_{u\\in V}\\mathbf{w}_{u\\to v}^{(l)}\\mathbf{\\bar x}_{u\\to v}^{(l-1)}+\\mathbf{b}_{v}^{(l)}\\right), \\forall v\\in V\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{\\bar x}_{u\\to v}^{(l-1)}=\\begin{cases}\n",
    "            0, & A_{u,v}=0\\\\\n",
    "            \\mathbf{x}_u^{(l-1)}, & A_{u,v}=1\n",
    "        \\end{cases}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "Assume that each feature is bounded, then the definition of $\\mathbf{\\bar x}_{u\\to v}^{(l-1)}$ could be reformulated using big-M:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{x}_{u}^{(l-1)}-\\mathbf{M}_{u}^{(l-1)}(1-A_{u,v})\\le &\\mathbf{\\bar x}_{u\\to v}^{(l-1)}\\le \\mathbf{x}_{u}^{(l-1)}+\\mathbf{M}_{u}^{(l-1)}(1-A_{u,v})\\\\\n",
    "        -\\mathbf{M}_{u}^{(l-1)}A_{u,v}\\le &\\mathbf{\\bar x}_{u\\to v}^{(l-1)}\\le \\mathbf{M}_u^{(l-1)}A_{u,v}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "where $|\\mathbf{x}_u^{(l-1)}|\\le \\mathbf{M}_u^{(l-1)}, A_{u,v}\\in\\{0,1\\}$. By adding extra continuous variables and constraints, as well as utilizing the bounds for all features, the big-M formulation replaces the bi-linear constraints by linear constraints.\n",
    "\n",
    "\n",
    "\n",
    "### Transforming a GNN with Non-fixed Graph Structure into a Dense NN\n",
    "\n",
    "Since the graph structure is unknown, all $\\mathbf{w}_{u\\to v}^{(l)},\\mathbf{b}_v^{(l)}$ should be provided. We reuse the previous example but this time the parameters in the Dense NN become:\n",
    "\n",
    "\\begin{equation*}\n",
    "        \\mathbf{W}=\\begin{pmatrix}\n",
    "            \\mathbf{w_1} & \\mathbf{w_2} & \\mathbf{w_2} \\\\\n",
    "            \\mathbf{w_2} & \\mathbf{w_1} & \\mathbf{w_2} \\\\\n",
    "            \\mathbf{w_2} & \\mathbf{w_2} & \\mathbf{w_1} \\\\\n",
    "        \\end{pmatrix},\n",
    "        \\mathbf{B}=\\begin{pmatrix}\n",
    "        \\mathbf{b}\\\\\\mathbf{b}\\\\\\mathbf{b}\n",
    "        \\end{pmatrix}\n",
    "    \\end{equation*}\n",
    "    \n",
    "Repeat all process before building formulation for the Dense NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph structure\n",
    "# number of nodes\n",
    "N = 3\n",
    "# adjacency matrix\n",
    "A = np.array([[1,1,1],[1,1,1],[1,1,1]])\n",
    "\n",
    "# in/out features\n",
    "in_features = 2\n",
    "out_features = 3\n",
    "\n",
    "# architecture of GNN\n",
    "# sage: in_features to out_features for each node, with ReLU as activation\n",
    "# add_pool: read out, sum out_features of each node\n",
    "# dense: out_features to 1\n",
    "gnn_layers = ['sage', 'add_pool', 'dense']\n",
    "activations = [True, False, False]\n",
    "\n",
    "# randomly generate GNN parameters from (-1,1)\n",
    "# in practice, these paprameters should be extracted from the trained GNN\n",
    "np.random.seed(123)\n",
    "sage_w1 = 2.* np.random.rand(out_features, in_features) -1.\n",
    "sage_w2 = 2.* np.random.rand(out_features, in_features) -1.\n",
    "sage_b = 2. * np.random.rand(out_features) - 1.\n",
    "\n",
    "dense_w = 2.* np.random.rand(1, out_features) - 1.\n",
    "dense_b = 2.* np.random.rand(1) - 1.\n",
    "\n",
    "params = []\n",
    "channels = []\n",
    "channels.append(N * in_features)\n",
    "\n",
    "for layer in gnn_layers:\n",
    "    if layer == 'sage':\n",
    "        params.append(SAGE_to_Dense(N,A,sage_w1,sage_w2,sage_b))\n",
    "        channels.append(sage_w1.shape[0] * N)\n",
    "    elif layer == 'dense':\n",
    "        params.append((dense_w,dense_b))\n",
    "        channels.append(w.shape[0])\n",
    "    elif layer == 'add_pool':\n",
    "        channels.append(channels[-1] // N)\n",
    "        w = np.zeros((channels[-1],channels[-2]))\n",
    "        for i in range(channels[-1]):\n",
    "            for j in range(N):\n",
    "                w[i, i+j*channels[-1]] = 1.\n",
    "        b = np.zeros(channels[-1])\n",
    "        params.append((w,b))\n",
    "        \n",
    "model_dense = PyTorchModel(len(channels)-1, params, activations)\n",
    "# print(model_dense)\n",
    "\n",
    "# for param in model_dense.parameters():\n",
    "#     print(param)\n",
    "\n",
    "dummy_input = torch.zeros(channels[0], dtype=torch.float64)\n",
    "dummy_input.requires_grad=True\n",
    "input_bounds = [(-1., 1.) for _ in range(channels[0])]\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as f:\n",
    "    #export neural network to ONNX\n",
    "    torch.onnx.export(\n",
    "        model_dense,\n",
    "        dummy_input,\n",
    "        f,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "    )\n",
    "    #write ONNX model and its bounds using OMLT\n",
    "    write_onnx_model_with_bounds(f.name, None, input_bounds)\n",
    "    #load the network definition from the ONNX model\n",
    "    network_definition = load_onnx_neural_network_with_bounds(f.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a MIP Formulation and Solve the Optimization Problem\n",
    "\n",
    "Note that all types of layers are represented as dense layers in OMLT now. Using `gnn_layer_definition` to retrieve GNN layers. The number of nodes `N` and the list of indexes for GNN layers `gnn_layers` should be provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tInputLayer(input_size=[6], output_size=[6])\tlinear\n",
      "1\tGNNLayer(input_size=[6], output_size=[9])\tlinear\n",
      "2\tDenseLayer(input_size=[9], output_size=[3])\tlinear\n",
      "3\tDenseLayer(input_size=[3], output_size=[1])\tlinear\n"
     ]
    }
   ],
   "source": [
    "# replace dense layers with GNN layers\n",
    "gnn_net = gnn_layer_definition(network_definition, N=N, gnn_layers=[1])\n",
    "    \n",
    "for layer_id, layer in enumerate(gnn_net.layers):\n",
    "    print(f\"{layer_id}\\t{layer}\\t{layer.activation}\")\n",
    "    \n",
    "formulation = ReluBigMFormulation(gnn_net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building formulation for GNN layers, one needs to define binary variables for adjacency matrix $A$, which is required when using `build_formulation` to encode GNN layers. \n",
    "\n",
    "Here we set the diagonal elements of $A$ be $1$ to guarantee the self contribution of each node. However, one can fix different elements in $A$ based on different problems. For example, fix most elements in $A$ and only optimize over a subset of edges. The extrame case is that fixing all elements, which is equivalent to the case with fixed graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.5 \n",
      "Build Date: Dec  8 2020 \n",
      "\n",
      "command line - /rds/general/user/sz421/home/anaconda3/envs/OMLT/bin/cbc -printingOptions all -import /var/tmp/pbs.7796016.pbs/tmp4_rnxuex.pyomo.lp -stat=1 -solve -solu /var/tmp/pbs.7796016.pbs/tmp4_rnxuex.pyomo.soln (default strategy 1)\n",
      "Option for printingOptions changed from normal to all\n",
      "Presolve 63 (-49) rows, 33 (-37) columns and 185 (-104) elements\n",
      "Statistics for presolved model\n",
      "Original problem has 6 integers (6 of which binary)\n",
      "Presolved problem has 6 integers (6 of which binary)\n",
      "==== 16 zero objective 8 different\n",
      "3 variables have objective of -0.649096\n",
      "1 variables have objective of -0.635017\n",
      "4 variables have objective of -0.268763\n",
      "16 variables have objective of 0\n",
      "2 variables have objective of 0.312969\n",
      "1 variables have objective of 0.475991\n",
      "2 variables have objective of 0.481896\n",
      "4 variables have objective of 0.533943\n",
      "==== absolute objective values 8 different\n",
      "16 variables have objective of 0\n",
      "4 variables have objective of 0.268763\n",
      "2 variables have objective of 0.312969\n",
      "1 variables have objective of 0.475991\n",
      "2 variables have objective of 0.481896\n",
      "4 variables have objective of 0.533943\n",
      "1 variables have objective of 0.635017\n",
      "3 variables have objective of 0.649096\n",
      "==== for integers 6 zero objective 1 different\n",
      "6 variables have objective of 0\n",
      "==== for integers absolute objective values 1 different\n",
      "6 variables have objective of 0\n",
      "===== end objective counts\n",
      "\n",
      "\n",
      "Problem has 63 rows, 33 columns (17 with objective) and 185 elements\n",
      "There are 3 singletons with objective \n",
      "Column breakdown:\n",
      "0 of type 0.0->inf, 0 of type 0.0->up, 0 of type lo->inf, \n",
      "27 of type lo->up, 0 of type free, 0 of type fixed, \n",
      "0 of type -inf->0.0, 0 of type -inf->up, 6 of type 0.0->1.0 \n",
      "Row breakdown:\n",
      "0 of type E 0.0, 0 of type E 1.0, 0 of type E -1.0, \n",
      "3 of type E other, 0 of type G 0.0, 0 of type G 1.0, \n",
      "0 of type G other, 32 of type L 0.0, 24 of type L 1.0, \n",
      "4 of type L other, 0 of type Range 0.0->1.0, 0 of type Range other, \n",
      "0 of type Free \n",
      "Continuous objective value is -2.55499 - 0.00 seconds\n",
      "Cgl0004I processed model has 64 rows, 34 columns (6 integer (6 of which binary)) and 184 elements\n",
      "Cbc0038I Initial state - 0 integers unsatisfied sum - 0\n",
      "Cbc0038I Solution found of -2.55499\n",
      "Cbc0038I Relaxing continuous gives -2.55499\n",
      "Cbc0038I Before mini branch and bound, 6 integers at bound fixed and 24 continuous\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.00 seconds)\n",
      "Cbc0038I After 0.00 seconds - Feasibility pump exiting with objective of -2.55499 - took 0.00 seconds\n",
      "Cbc0012I Integer solution of -2.5549939 found by feasibility pump after 0 iterations and 0 nodes (0.00 seconds)\n",
      "Cbc0001I Search completed - best objective -2.554993930144489, took 0 iterations and 0 nodes (0.00 seconds)\n",
      "Cbc0035I Maximum depth 0, 0 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from -2.55499 to -2.55499\n",
      "Probing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                -2.55499393\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.00\n",
      "Time (Wallclock seconds):       0.00\n",
      "\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Problem': [{'Name': 'unknown', 'Lower bound': -2.55499393, 'Upper bound': -2.55499393, 'Number of objectives': 1, 'Number of constraints': 63, 'Number of variables': 33, 'Number of binary variables': 6, 'Number of integer variables': 6, 'Number of nonzeros': 17, 'Sense': 'minimize'}], 'Solver': [{'Status': 'ok', 'User time': -1.0, 'System time': 0.0, 'Wallclock time': 0.0, 'Termination condition': 'optimal', 'Termination message': 'Model was solved to optimality (subject to tolerances), and an optimal solution is available.', 'Statistics': {'Branch and bound': {'Number of bounded subproblems': 0, 'Number of created subproblems': 0}, 'Black box': {'Number of iterations': 0}}, 'Error rc': 0, 'Time': 0.03782343864440918}], 'Solution': [OrderedDict([('number of solutions', 0), ('number of solutions displayed', 0)])]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pyomo model\n",
    "m = pyo.ConcreteModel()\n",
    "\n",
    "# create an OMLT block for the neural network and build its formulation\n",
    "m.nn = OmltBlock()\n",
    "\n",
    "# initialize graph information\n",
    "m.nn.A = pyo.Var(\n",
    "    pyo.Set(initialize=range(N)), pyo.Set(initialize=range(N)), within=pyo.Binary\n",
    ")\n",
    "# usually, the contribution from node v to itself exists\n",
    "for i in range(N):\n",
    "    m.nn.A[i, i].fix(1)\n",
    "\n",
    "m.nn.build_formulation(formulation)\n",
    "\n",
    "m.obj = pyo.Objective(expr=(m.nn.outputs[0]))\n",
    "pyo.SolverFactory(\"cbc\").solve(m, tee=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "For cases with fixed graph structure, one needs to transform the trained GNN into a Dense NN before using OMLT. After the transformation step, optimizing over a trained GNN is equivalent to optimizing over the corresponding Dense NN. No extra action is needed when using OMLT to encode the Dense NN.\n",
    "\n",
    "For cases with non-fixed graph structure, the following actions are required:\n",
    "\n",
    "- providing all $\\mathbf{w}_{u\\to v}^{(l)},\\mathbf{b}_v^{(l)}$ in transformation step since any of them could be used.\n",
    "- using `gnn_layer_definition` to retrieve GNN layers after loading ONNX model. The number of nodes in graph `N` and the list of indexes for GNN layers `gnn_layers` should be provided here.\n",
    "- defining binary variables for adjacency matrix $A$ before using `build_formulation` since these variables are used to formulate GNN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:OMLT]",
   "language": "python",
   "name": "conda-env-OMLT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
