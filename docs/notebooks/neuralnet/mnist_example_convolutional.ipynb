{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Optimal adversaries for convolutional MNIST model\n",
    "\n",
    "This notebook gives an example where OMLT is used to find adversarial examples for a trained convolutional neural network. We follow the below steps:<br>\n",
    "1.) A convolutional neural network (CNN) with ReLU activation functions is trained to classify images from the MNIST dataset <br>\n",
    "2.) OMLT is used to generate a mixed-integer encoding of the trained CNN using the big-M formulation <br>\n",
    "3.) The model is optimized to find the maximum classification error (defined by an \"adversarial\" label) over a small input region <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Setup\n",
    "This notebook assumes you have a working PyTorch environment to train the neural network for classification. The neural network is then formulated in Pyomo using OMLT which therefore requires working Pyomo and OMLT installations.\n",
    "\n",
    "The required Python libraries used this notebook are as follows: <br>\n",
    "- `numpy`: used for manipulate input data <br>\n",
    "- `torch`: the machine learning language we use to train our neural network\n",
    "- `torchvision`: a package containing the MNIST dataset\n",
    "- `pyomo`: the algebraic modeling language for Python, it is used to define the optimization model passed to the solver\n",
    "- `omlt`: the package this notebook demonstates. OMLT can formulate machine learning models (such as neural networks) within Pyomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import requisite packages\n",
    "#data manipulation\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "#pytorch for training neural network\n",
    "import torch, torch.onnx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "#pyomo for optimization\n",
    "import pyomo.environ as pyo\n",
    "\n",
    "#omlt for interfacing our neural network with pyomo\n",
    "from omlt import OmltBlock\n",
    "from omlt.neuralnet import FullSpaceNNFormulation\n",
    "from omlt.io.onnx import write_onnx_model_with_bounds, load_onnx_neural_network_with_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data and Train a Neural Network\n",
    "\n",
    "We begin by loading the MNIST dataset as `DataLoader` objects with pre-set training and testing batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training and test batch sizes\n",
    "train_kwargs = {'batch_size': 64}\n",
    "test_kwargs = {'batch_size': 1000}\n",
    "\n",
    "#build DataLoaders for training and test sets\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "dataset2 = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the structure of the convolutional neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #define layers of neural network\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1  = nn.Conv2d(1, 4, (4,4), (2,2), 0)\n",
    "        self.conv2  = nn.Conv2d(4, 4, (4,4), (2,2), 0)\n",
    "        self.hidden1 = nn.Linear(5*5*4, hidden_size)\n",
    "        self.output  = nn.Linear(hidden_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define forward pass of neural network\n",
    "    def forward(self, x):\n",
    "        self.x1 = self.conv1(x)\n",
    "        self.x2 = self.relu(self.x1)\n",
    "        self.x3 = self.conv2(self.x2)\n",
    "        self.x4 = self.relu(self.x3)\n",
    "        self.x5 = self.hidden1(self.x4.view((-1,5*5*4)))\n",
    "        self.x6 = self.relu(self.x5)\n",
    "        self.x7 = self.output(self.x6)\n",
    "        x = self.softmax(self.x7)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define simple functions for training and testing the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function computes loss and its gradient on batch, and prints status after every 200 batches\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train(); criterion = nn.NLLLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "#testing function computes loss and prints overall model accuracy on test set\n",
    "def test(model, test_loader):\n",
    "    model.eval(); criterion = nn.NLLLoss(reduction='sum')\n",
    "    test_loss = 0; correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  \n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the neural network on the dataset.\n",
    "Training here is performed using the `Adadelta` optimizer for five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.315399\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.334257\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.249197\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.264119\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.095784\n",
      "\n",
      "Test set: Average loss: 0.1388, Accuracy: 9571/10000 (96%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.152225\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.068908\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.336652\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.053365\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.250328\n",
      "\n",
      "Test set: Average loss: 0.1090, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.102334\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.061703\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.256420\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.023817\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.089454\n",
      "\n",
      "Test set: Average loss: 0.0893, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.052203\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.076382\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.085071\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.048361\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.046855\n",
      "\n",
      "Test set: Average loss: 0.0821, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.018508\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.041259\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.029856\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.064394\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.022107\n",
      "\n",
      "Test set: Average loss: 0.0742, Accuracy: 9767/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define model and optimizer\n",
    "model = Net()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "#train CNN model for five epochs\n",
    "for epoch in range(5):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a MIP Formulation for the Trained Convolutional Neural Network\n",
    "\n",
    "We are now ready to use OMLT to formulate the trained model within a Pyomo optimization model. The nonsmooth ReLU activation function requires using a full-space representation, which uses the `NeuralNetworkFormulation` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a neural network without the final `LogSoftmax` activation. Although this activation helps greatly in training the neural network model, it is not trivial to encode in the optimization model. The ranking of the output labels remains the same without the activation, so it can be omitted when finding optimal adversaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NoSoftmaxNet(nn.Module):\n",
    "    #define layers of neural network\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1  = nn.Conv2d(1, 4, (4,4), (2,2), 0)\n",
    "        self.conv2  = nn.Conv2d(4, 4, (4,4), (2,2), 0)\n",
    "        self.hidden1 = nn.Linear(5 * 5 * 4, hidden_size)\n",
    "        self.output  = nn.Linear(hidden_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    #define forward pass of neural network\n",
    "    def forward(self, x):\n",
    "        self.x1 = self.conv1(x)\n",
    "        self.x2 = self.relu(self.x1)\n",
    "        self.x3 = self.conv2(self.x2)\n",
    "        self.x4 = self.relu(self.x3)\n",
    "        self.x5 = self.hidden1(self.x4.view((-1,5*5*4)))\n",
    "        self.x6 = self.relu(self.x5)\n",
    "        x = self.output(self.x6)    \n",
    "        return x\n",
    "\n",
    "#create neural network without LogSoftmax and load parameters from existing model\n",
    "model2 = NoSoftmaxNet()\n",
    "model2.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an instance of the optimal adversary problem. We formulate the optimization problem as: <br>\n",
    "\n",
    "$\n",
    "\\begin{align*} \n",
    "& \\max_x \\ y_k - y_j \\\\\n",
    "& s.t. y_k = N_k(x) \\\\ \n",
    "&\\quad |x - \\bar{x}|_\\infty \\leq 0.05\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $\\bar{x}$ corresponds to an image in the test dataset with true label `j`, and $N_k(x)$ is the value of the CNN output corresponding to adversarial label `k` given input `x`. PyTorch needs to trace the model execution to export it to ONNX, so we also define a dummy input tensor `x_temp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image and true label from test set with index 'problem_index'\n",
    "problem_index = 0\n",
    "image = dataset2[problem_index][0].detach().numpy()\n",
    "label = dataset2[problem_index][1]\n",
    "\n",
    "#define input region defined by infinity norm\n",
    "epsilon_infty = 1e-3\n",
    "lb = np.maximum(0, image - epsilon_infty)\n",
    "ub = np.minimum(1, image + epsilon_infty)\n",
    "\n",
    "#save input bounds as dictionary, note that the first index 0 corresponds to the single-channel input\n",
    "input_bounds = {}\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        input_bounds[(0,i,j)] = (float(lb[0][i,j]), float(ub[0][i,j])) \n",
    "    \n",
    "#define dummy input tensor    \n",
    "x = dataset2[problem_index][0].view(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now export the PyTorch model as an ONNX model and use `load_onnx_neural_network_with_bounds` to load it into OMLT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as f:\n",
    "    #export neural network to ONNX\n",
    "    torch.onnx.export(\n",
    "        model2,\n",
    "        x,\n",
    "        f,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    #write ONNX model and its bounds using OMLT\n",
    "    write_onnx_model_with_bounds(f.name, None, input_bounds)\n",
    "    #load the network definition from the ONNX model\n",
    "    network_definition = load_onnx_neural_network_with_bounds(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check before creating the optimization model, we can print the properties of the neural network layers from `network_definition`. This allows us to check input/output sizes, as well as activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tInputLayer(input_size=[1, 28, 28], output_size=[1, 28, 28])\tlinear\n",
      "1\tConvLayer(input_size=[1, 28, 28], output_size=[4, 13, 13], strides=[2, 2], kernel_shape=(4, 4))\trelu\n",
      "2\tConvLayer(input_size=[4, 13, 13], output_size=[4, 5, 5], strides=[2, 2], kernel_shape=(4, 4))\trelu\n",
      "3\tDenseLayer(input_size=[1, 100], output_size=[1, 50])\trelu\n",
      "4\tDenseLayer(input_size=[1, 50], output_size=[1, 10])\tlinear\n"
     ]
    }
   ],
   "source": [
    "for layer_id, layer in enumerate(network_definition.layers):\n",
    "    print(f\"{layer_id}\\t{layer}\\t{layer.activation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can load `network_definition` as a full-space `FullSpaceNNFormulation` object.OMLT doesn't include a formulation for sigmoid, so define it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "formulation = FullSpaceNNFormulation(network_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Optimal Adversary Problem in Pyomo\n",
    "\n",
    "We now encode the trained neural network in a Pyomo model from the `FullSpaceNNFormulation` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pyomo model\n",
    "m = pyo.ConcreteModel()\n",
    "\n",
    "#create an OMLT block for the neural network and build its formulation\n",
    "m.nn = OmltBlock()\n",
    "m.nn.build_formulation(formulation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an adversarial label as the true label plus one (or zero if the true label is nine), as well as the objective function for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = (label + 1) % 10\n",
    "m.obj = pyo.Objective(expr=(-(m.nn.outputs[0,adversary]-m.nn.outputs[0,label])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we solve the optimal adversary problem using a mixed-integer solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2023-01-12\n",
      "Using license file /Users/calvintsay/gurobi.lic\n",
      "Read LP format model from file /var/folders/pc/7mzx4b956_lb2l8_ryngwydc0000gn/T/tmpcs4b0910.pyomo.lp\n",
      "Reading time = 0.04 seconds\n",
      "x4871: 5739 rows, 4871 columns, 33357 nonzeros\n",
      "Changed value of parameter mipgap to 0.01\n",
      "   Prev: 0.0001  Min: 0.0  Max: inf  Default: 0.0001\n",
      "Gurobi Optimizer version 9.1.1 build v9.1.1rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 5739 rows, 4871 columns and 33357 nonzeros\n",
      "Model fingerprint: 0xe52cb635\n",
      "Variable types: 4045 continuous, 826 integer (826 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [2e-05, 3e+01]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [2e-04, 1e+02]\n",
      "  RHS range        [2e-04, 2e+01]\n",
      "Presolve removed 3931 rows and 2839 columns\n",
      "Presolve time: 0.05s\n",
      "Presolved: 1808 rows, 2032 columns, 11310 nonzeros\n",
      "Variable types: 1580 continuous, 452 integer (452 binary)\n",
      "\n",
      "Root relaxation: objective 1.463077e+01, 675 iterations, 0.01 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   14.63077    0  213          -   14.63077      -     -    0s\n",
      "     0     0   14.63282    0  215          -   14.63282      -     -    0s\n",
      "     0     0   14.63288    0  213          -   14.63288      -     -    0s\n",
      "H    0     0                      14.7420840   14.63288  0.74%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 2\n",
      "  Flow cover: 13\n",
      "  RLT: 6\n",
      "\n",
      "Explored 1 nodes (765 simplex iterations) in 0.22 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 14.7421 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-02)\n",
      "Best objective 1.474208404275e+01, best bound 1.463288183519e+01, gap 0.7408%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Problem': [{'Name': 'x4871', 'Lower bound': 14.632881835188687, 'Upper bound': 14.742084042745333, 'Number of objectives': 1, 'Number of constraints': 5739, 'Number of variables': 4871, 'Number of binary variables': 826, 'Number of integer variables': 826, 'Number of continuous variables': 4045, 'Number of nonzeros': 33357, 'Sense': 'minimize'}], 'Solver': [{'Status': 'ok', 'Return code': '0', 'Message': 'Model was solved to optimality (subject to tolerances), and an optimal solution is available.', 'Termination condition': 'optimal', 'Termination message': 'Model was solved to optimality (subject to tolerances), and an optimal solution is available.', 'Wall time': '0.21825098991394043', 'Error rc': 0, 'Time': 0.4484848976135254}], 'Solution': [OrderedDict([('number of solutions', 0), ('number of solutions displayed', 0)])]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = pyo.SolverFactory('gurobi')\n",
    "solver.options['mipgap'] = 0.01\n",
    "solver.solve(m, tee=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8f31a8284ce774e9ad8d309790c576c984c0620550967f9ef361ac8e66f487d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
