{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing over trained graph neural networks\n",
    "\n",
    "This notebook explains how OMLT is used to optimize over trained graph neural networks (GNNs). We follow the below steps:\n",
    "\n",
    "1.) A general definition of GNNs is provided. OMLT currently only supports GNN that fits our definition. \n",
    "\n",
    "2.) Introduce the `GNNLayer` class inside OMLT.\n",
    "\n",
    "3.) Derive the big-M formulation for GNN layers.\n",
    "\n",
    "4.) List operations that are implemented inside OMLT. OMLT can automatically encode a given GNN consists of these operations. \n",
    "\n",
    "5.) For customized GNNs, we give examples to illustrate how to transform it into OMLT. \n",
    "\n",
    "6.) Examples: one has fixed graph structure, another one has non-fixed graph structure. For each case, the output of the GNN is minimized.\n",
    "\n",
    "**NOTE:** For simplicity, we skip the training process and just use random parameters for GNNs.\n",
    "\n",
    "\n",
    "## Library Setup\n",
    "\n",
    "This notebook assumes you have a working PyTorch environment to define a Dense NN. This Dense NN is then formulated in Pyomo using OMLT which therefore requires working Pyomo and OMLT installations.\n",
    "\n",
    "The required Python libraries used in this notebook are as follows:\n",
    "\n",
    "- `numpy`: used for transformation of parameters\n",
    "\n",
    "- `torch`: the machine learning language used for neural networks\n",
    "\n",
    "- `torch_geometric`: the machine learning language used for graph neural networks\n",
    "\n",
    "- `pyomo`: the algebraic modeling language for Python, it is used to define the optimization model passed to the solver\n",
    "\n",
    "- `onnx`: used to express trained neural network models\n",
    "\n",
    "- `omlt`: the package this notebook demonstrates. OMLT can formulate machine learning (such as neural networks) within Pyomo\n",
    "\n",
    "**NOTE:** This notebook also assumes you have a working MIP solver executable to solve optimization problems in Pyomo. The open-source solver CBC is called by default. \n",
    "\n",
    "\n",
    "## Definition of GNNs\n",
    "\n",
    "We define a GNN with $L$ layers as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tGNN:\\underbrace{\\mathbb R^{d_0}\\otimes\\cdots\\otimes\\mathbb R^{d_0}}_{N \\rm{times}}\\to\\underbrace{\\mathbb R^{d_L}\\otimes\\cdots\\otimes\\mathbb R^{d_L}}_{N\\ \\rm{times}}\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "    \n",
    "where $V$ is the set of nodes of the input graph, $N=|V|$ is the number of nodes. \n",
    "\n",
    "Let $\\mathbf{x}_v^{(0)} \\in \\mathbb{R}^{d_0}$ be the input features for node $v$. Then, the $l$-th layer ($l=1,2,\\dots,L$) is defined by:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\mathbf{x}_v^{(l)}=\\sigma\\left(\\sum\\limits_{u\\in\\mathcal N(v)\\cup\\{v\\}}\\mathbf{w}_{u\\to v}^{(l)}\\mathbf{x}_u^{(l-1)}+\\mathbf{b}_{v}^{(l)}\\right),~\\forall v\\in V\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathcal N(v)$ is the set of all neighbors of $v$, $\\sigma$ could be identity or any activation function.\n",
    "\n",
    "*Dimensionality:* $\\mathbf{x}_u^{(l-1)}\\in\\mathbb R^{d_{l-1}}, \\mathbf{x}_v^{(l)},\\mathbf{b}_v^{(l)}\\in\\mathbb R^{d_l}, \\mathbf{w}_{u\\to v}^{(l)}\\in\\mathbb R^{d_l}\\times \\mathbb R^{d_{l-1}}$.\n",
    "\n",
    "## GNN Layers in OMLT\n",
    "\n",
    "For optimization purposes, OMLT requires a given number of nodes $N$ in the input graph. Each GNN layer will be expanded as shown in follows.\n",
    "\n",
    "Stack $\\{\\mathbf{x}_v^{(l)}\\}_{v\\in V}$ as a vector $\\mathbf{X}^{(l)}\\in \\mathbb R^{Nd_l}$. Rewrite previous definition as:\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\mathbf{X}^{(l)}=\\sigma\\left(\\mathbf{W}^{(l)}\\mathbf{X}^{(l-1)}+\\mathbf{B}^{(l)}\\right)\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "One needs to provide $\\mathbf{W}^{(l)}\\in\\mathbb R^{Nd_l\\times Nd_{l-1}}, \\mathbf{B}^{(l)}\\in\\mathbb R^{Nd_l}, N$ to define a `GNNLayer` in OMLT. \n",
    "\n",
    "**NOTE:** To keep consistency with other types of layers, all weights are transposed.\n",
    "\n",
    "If the input graph structure is fixed, then weights $\\mathbf{w}_{u\\to v}^{(l)}$, biases $\\mathbf{b}_{v}^{(l)}$, and links between layers determined by $\\mathcal N(v)$ are all fixed after the GNN is trained. In this case, $\\mathbf{W}^{(l)}$ is a sparse matrix with nonzero sub-matrices $\\{\\mathbf{w}_{u\\to v}^{(l)}\\}_{v\\in V,u\\in\\mathcal N(v)\\cup\\{v\\}}$ and $\\mathbf{B}^{(l)}$ is the stack of $\\{\\mathbf{b}_v^{(l)}\\}_{v\\in V}$. The mixed-integer formulation of `GNNLayer` can be interpreted from two perspectives: (1) the same as a `DenseLayer`; (2) a simplified setting of `GNNLayer` with non-fixed input graph (introduced later).   \n",
    "\n",
    "If the input graph structure is not fixed, then all weights $\\mathbf{w}_{u\\to v}^{(l)}$ and biases $\\mathbf{b}_{v}^{(l)}$ are needed to build mixed-integer formulations. In this case, $\\mathbf{B}^{(l)}$ is still the stack of $\\{\\mathbf{b}_v^{(l)}\\}_{v\\in V}$, while $\\mathbf{W}^{(l)}$ is a dense matrix consists of $\\{\\mathbf{w}_{u\\to v}^{(l)}\\}_{u,v\\in V}$.\n",
    "\n",
    "## Formulation for GNN Layers\n",
    "\n",
    "When the input graph structure is not fixed, elements in the adjacency matrix $A$ are decision variables. In this case, $\\mathcal N(v)$ is not given anymore. Additionally, $\\mathbf{w}_{u\\to v}^{(l)},\\mathbf{b}_v^{(l)}$ may contain the graph information, which makes them be variables. The formulation for GNN layers is built assuming that $\\mathbf{w}_{u\\to v}^{(l)},\\mathbf{b}_v^{(l)}$ are fixed.\n",
    "\n",
    "First, observe that the existence of edge $u\\to v$ determines the contribution link from $\\mathbf{x}_u^{(l-1)}$ to $\\mathbf{x}_v^{(l)}$. Adding binary variables $A_{u,v}$ for all $u,v\\in V$, we can formulate GNNs in a bilinear way:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{x}_v^{(l)}=\\sigma\\left(\\sum\\limits_{u\\in V}A_{u,v}\\mathbf{w}_{u\\to v}^{(l)}\\mathbf{x}_u^{(l-1)}+\\mathbf{b}_{v}^{(l)}\\right), \\forall v\\in V\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "This formulation involves quadratic constraints. To avoid them, instead of using binary variables to directly control the existence of contributions between nodes, we introduce introduces auxiliary variables $\\mathbf{\\bar x}_{u\\to v}^{(l-1)}$ to represent the contribution from node $u$ to node $v$ in $l$-th layer:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{x}_v^{(l)}=\\sigma\\left(\\sum\\limits_{u\\in V}\\mathbf{w}_{u\\to v}^{(l)}\\mathbf{\\bar x}_{u\\to v}^{(l-1)}+\\mathbf{b}_{v}^{(l)}\\right), \\forall v\\in V\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{\\bar x}_{u\\to v}^{(l-1)}=\\begin{cases}\n",
    "            0, & A_{u,v}=0\\\\\n",
    "            \\mathbf{x}_u^{(l-1)}, & A_{u,v}=1\n",
    "        \\end{cases}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "Assume that each feature is bounded, then the definition of $\\mathbf{\\bar x}_{u\\to v}^{(l-1)}$ could be reformulated using big-M:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{x}_{u}^{(l-1)}-\\mathbf{M}_{u}^{(l-1)}(1-A_{u,v})\\le &~\\mathbf{\\bar x}_{u\\to v}^{(l-1)}\\le \\mathbf{x}_{u}^{(l-1)}+\\mathbf{M}_{u}^{(l-1)}(1-A_{u,v})\\\\\n",
    "        -\\mathbf{M}_{u}^{(l-1)}A_{u,v}\\le &~\\mathbf{\\bar x}_{u\\to v}^{(l-1)}\\le \\mathbf{M}_u^{(l-1)}A_{u,v}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "where $|\\mathbf{x}_u^{(l-1)}|\\le \\mathbf{M}_u^{(l-1)}, A_{u,v}\\in\\{0,1\\}$. By adding extra continuous variables and constraints, as well as utilizing the bounds for all features, the big-M formulation replaces the bi-linear constraints by linear constraints. OMLT uses this big-M formulation as the default (and only) formulation for GNN layers.\n",
    "\n",
    "**NOTE:** When the input graph structure is fixed, we can fix $A$ to reduce the big-M formulation.\n",
    "\n",
    "## Implemented GNN Operations in OMLT\n",
    "\n",
    "The following operations from `torch_geometric` are implemented in OMLT:\n",
    "\n",
    "- Convolutional Layers: `Linear`, `GCNConv`, `SAGEConv`\n",
    "- Aggregation Operators: `sum`, `mean`\n",
    "- Pooling Layers: `global_mean_pool`, `global_add_pool`\n",
    "- Activation Functions: all activations supported in OMLT are compatible with these GNN operations.\n",
    "\n",
    "**NOTE:** When the input graph is not fixed, there is no graph information. `GCNConv` layer and `mean` aggregation are not supported.\n",
    "\n",
    "OMLT provides two functions `gnn_with_fixed_graph` and `gnn_with_non_fixed_graph` to encode GNNs with fixed/non-fixed input graph structure. Both functions only require (1) a sequential model from `torch_geometric`, and (2) number of nodes $N$ (and the adjacency matrix $A$ for fixed graph cases). The basic pipeline of both functions are:\n",
    "\n",
    "1.) Transform each operation, e.g., linear layers and pooling layers will be transformed into `DenseLayer` (which is straightforward), GNN layers will be rewritten into `GNNLayer`, activation functions are identified and absorbed into corresponding layers.\n",
    "\n",
    "2.) Define binary variables $A_{u,v}$ for adjacency matrix. We always assume $A$ is symmetric (i.e., $A_{u,v}=A_{v,u}$) and has non-zero diagonal elements (i.e., $A_{v,v}=1$). When $A$ is given for fixed graph cases, these variables will then be fixed.\n",
    "\n",
    "3.) Build formulation. Currently, we only support `FullSpaceNNFormulations`. ReLU activation functions are encoded into linear constraints using a big-M formulation. For smooth activation functions (e.g., Sigmoid, LogSoftmax, Tanh), a smooth optimization solvers (such as Ipopt) is needed to handle nonlinear constraints.\n",
    "\n",
    "## How to Transform Your Own GNN into OMLT\n",
    "\n",
    "As mentioned before, any GNN that satisfies our GNN definition could be transformed into OMLT and then encoded using big-M formulation. Here we give two examples to show how to transform an outside GNN into OMLT.\n",
    "\n",
    "The first example corresponds to fixed graph cases. Given a simple GNN consists of a GraphSAGE layer, an add pooling layer, and a dense layer with single output. Let the input and output features of the GraphSAGE layer are 2 and 3, respectively. \n",
    "\n",
    "The GraphSAGE layer is defined by:\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}_v^{(l)}=\\sigma\\left(\\mathbf{w_1}^{(l)}\\mathbf{x}_v^{(l-1)}+\\mathbf{w_2}^{(l)}\\sum\\limits_{u\\in\\mathcal N(v)}\\mathbf{x}_u^{(l-1)}+\\mathbf{b}^{(l)}\\right)\n",
    "\\end{equation*}\n",
    "where a sum aggregation is used.\n",
    "\n",
    "For the fixed graph structure, assume that it is a line graph with $N=3$ nodes, i.e., the adjacency matrix $A=\\begin{pmatrix}1 & 1 & 0\\\\1 & 1 & 1\\\\ 0 & 1 & 1\\end{pmatrix}$. Then the GraphSAGE layer could be rewritten as a `GNNLayer` with parameters:\n",
    "\n",
    "   \\begin{equation*}\n",
    "        \\mathbf{W}=\\begin{pmatrix}\n",
    "            \\mathbf{w_1} & \\mathbf{w_2} & \\mathbf{0} \\\\\n",
    "            \\mathbf{w_2} & \\mathbf{w_1} & \\mathbf{w_2} \\\\\n",
    "            \\mathbf{0} & \\mathbf{w_2} & \\mathbf{w_1} \\\\\n",
    "        \\end{pmatrix},\n",
    "        \\mathbf{B}=\\begin{pmatrix}\n",
    "        \\mathbf{b}\\\\\\mathbf{b}\\\\\\mathbf{b}\n",
    "        \\end{pmatrix}\n",
    "    \\end{equation*}\n",
    "    \n",
    "See below as a mapping between the given outside GNN and its corresponding GNN inside OMLT in form \"layer type (in_channel, out_channel)\":\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\text{GraphSAGE(2, 3)}   &\\Rightarrow \\text{GNNLayer(6, 9)}\\\\\n",
    "        \\text{add pooling(9, 3)}  &\\Rightarrow \\text{DenseLayer(9, 3)}\\\\\n",
    "        \\text{dense(3, 1)}      &\\Rightarrow \\text{DenseLayer(3, 1)}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "The second example reuses the GNN architecture but no longer fixes the input graph structure. In such setting, all $\\mathbf{w}_{u\\to v}^{(l)},\\mathbf{b}_v^{(l)}$ should be provided. Therefore, the `GNNLayer` is defined by:\n",
    "\n",
    "\\begin{equation*}\n",
    "        \\mathbf{W}=\\begin{pmatrix}\n",
    "            \\mathbf{w_1} & \\mathbf{w_2} & \\mathbf{w_2} \\\\\n",
    "            \\mathbf{w_2} & \\mathbf{w_1} & \\mathbf{w_2} \\\\\n",
    "            \\mathbf{w_2} & \\mathbf{w_2} & \\mathbf{w_1} \\\\\n",
    "        \\end{pmatrix},\n",
    "        \\mathbf{B}=\\begin{pmatrix}\n",
    "        \\mathbf{b}\\\\\\mathbf{b}\\\\\\mathbf{b}\n",
    "        \\end{pmatrix}\n",
    "    \\end{equation*}\n",
    "\n",
    "## Example 1: Optimizing a GNN with Fixed Graph\n",
    "\n",
    "Define a GCN in `torch_geometric` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.nn import Sequential, GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from omlt.io.torch_geometric import gnn_with_fixed_graph\n",
    "import pyomo.environ as pyo\n",
    "from omlt import OmltBlock\n",
    "\n",
    "\n",
    "def GCN_Sequential(activation, pooling):\n",
    "    torch.manual_seed(123)\n",
    "    return Sequential(\n",
    "        \"x, edge_index\",\n",
    "        [\n",
    "            (GCNConv(2, 4), \"x, edge_index -> x\"),\n",
    "            activation(),\n",
    "            (GCNConv(4, 4), \"x, edge_index -> x\"),\n",
    "            activation(),\n",
    "            Linear(4, 4),\n",
    "            (pooling, \"x, None -> x\"),\n",
    "            Linear(4, 2),\n",
    "            activation(),\n",
    "            Linear(2, 1),\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has two types of `Linear` layers: the first linear layer maps in-features to out-features for each node, the last two linear layers map features after pooling. For illustration purposes, we use `load_torch_geometric_sequential` to show the transformed model in OMLT (this step is not needed for later formulation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tInputLayer(input_size=[6], output_size=[6])\tlinear\n",
      "1\tGNNLayer(input_size=[6], output_size=[12])\trelu\n",
      "2\tGNNLayer(input_size=[12], output_size=[12])\trelu\n",
      "3\tDenseLayer(input_size=[12], output_size=[12])\tlinear\n",
      "4\tDenseLayer(input_size=[12], output_size=[4])\tlinear\n",
      "5\tDenseLayer(input_size=[4], output_size=[2])\trelu\n",
      "6\tDenseLayer(input_size=[2], output_size=[1])\tlinear\n"
     ]
    }
   ],
   "source": [
    "from omlt.io.torch_geometric import load_torch_geometric_sequential\n",
    "\n",
    "# define a GCN sequential model\n",
    "nn = GCN_Sequential(ReLU, global_mean_pool)\n",
    "# number of nodes\n",
    "N = 3\n",
    "# adjacency matrix\n",
    "A = np.array([[1, 1, 0], [1, 1, 1], [0, 1, 1]])\n",
    "\n",
    "# load the model into OMLT\n",
    "net = load_torch_geometric_sequential(nn, N, A)\n",
    "\n",
    "for layer_id, layer in enumerate(net.layers):\n",
    "    print(f\"{layer_id}\\t{layer}\\t{layer.activation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two GCN layers are rewritten into two `GNNLayer` in OMLT given $N$ and $A$. The first linear layer is expanded since it maps features of each node. The pooling layer is equivalently transformed into a `DenseLayer`. The last two linear layers are the same as before since features of each node are pooled.\n",
    "\n",
    "Besides giving $N$ and $A$, one needs to define bounds for inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a GCN sequential model\n",
    "nn1 = GCN_Sequential(ReLU, global_mean_pool)\n",
    "# number of nodes\n",
    "N = 3\n",
    "# adjacency matrix\n",
    "A = np.array([[1, 1, 0], [1, 1, 1], [0, 1, 1]])\n",
    "\n",
    "# size of inputs = number of nodes x number of input features\n",
    "input_size = [6]\n",
    "# define lower and upper bounds for each input\n",
    "input_bounds = {}\n",
    "for i in range(input_size[0]):\n",
    "    input_bounds[(i)] = (-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having these information, the last step is to create an `OmltBlock` and build formulation in this block using `gnn_with_fixed_graph`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.10 \n",
      "Build Date: Aug  1 2023 \n",
      "\n",
      "command line - /rds/general/user/sz421/home/anaconda3/envs/OMLT_test/bin/cbc -printingOptions all -import /var/tmp/pbs.8152010.pbs/tmpo87uiyi0.pyomo.lp -stat=1 -solve -solu /var/tmp/pbs.8152010.pbs/tmpo87uiyi0.pyomo.soln (default strategy 1)\n",
      "Option for printingOptions changed from normal to all\n",
      "Presolve 172 (-222) rows, 111 (-75) columns and 608 (-267) elements\n",
      "Statistics for presolved model\n",
      "Original problem has 26 integers (26 of which binary)\n",
      "Presolved problem has 25 integers (25 of which binary)\n",
      "==== 110 zero objective 2 different\n",
      "1 variables have objective of -0.0421598\n",
      "110 variables have objective of 0\n",
      "==== absolute objective values 2 different\n",
      "110 variables have objective of 0\n",
      "1 variables have objective of 0.0421598\n",
      "==== for integers 25 zero objective 1 different\n",
      "25 variables have objective of 0\n",
      "==== for integers absolute objective values 1 different\n",
      "25 variables have objective of 0\n",
      "===== end objective counts\n",
      "\n",
      "\n",
      "Problem has 172 rows, 111 columns (1 with objective) and 608 elements\n",
      "Column breakdown:\n",
      "0 of type 0.0->inf, 49 of type 0.0->up, 0 of type lo->inf, \n",
      "37 of type lo->up, 0 of type free, 0 of type fixed, \n",
      "0 of type -inf->0.0, 0 of type -inf->up, 25 of type 0.0->1.0 \n",
      "Row breakdown:\n",
      "8 of type E 0.0, 0 of type E 1.0, 0 of type E -1.0, \n",
      "5 of type E other, 0 of type G 0.0, 0 of type G 1.0, \n",
      "0 of type G other, 134 of type L 0.0, 0 of type L 1.0, \n",
      "25 of type L other, 0 of type Range 0.0->1.0, 0 of type Range other, \n",
      "0 of type Free \n",
      "Continuous objective value is 0.315152 - 0.00 seconds\n",
      "Cgl0003I 0 fixed, 0 tightened bounds, 2 strengthened rows, 0 substitutions\n",
      "Cgl0004I processed model has 166 rows, 105 columns (25 integer (25 of which binary)) and 670 elements\n",
      "Cbc0038I Initial state - 5 integers unsatisfied sum - 0.124759\n",
      "Cbc0038I Pass   1: suminf.    0.00000 (0) obj. 0.317969 iterations 41\n",
      "Cbc0038I Solution found of 0.317969\n",
      "Cbc0038I Relaxing continuous gives 0.317969\n",
      "Cbc0038I Before mini branch and bound, 20 integers at bound fixed and 63 continuous\n",
      "Cbc0038I Full problem 166 rows 105 columns, reduced to 17 rows 13 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.317791\n",
      "Cbc0038I Pass   2: suminf.    0.00876 (1) obj. 0.317791 iterations 11\n",
      "Cbc0038I Pass   3: suminf.    0.18897 (1) obj. 0.317791 iterations 20\n",
      "Cbc0038I Pass   4: suminf.    0.00876 (1) obj. 0.317791 iterations 58\n",
      "Cbc0038I Pass   5: suminf.    0.18897 (1) obj. 0.317791 iterations 13\n",
      "Cbc0038I Pass   6: suminf.    0.00876 (1) obj. 0.317791 iterations 21\n",
      "Cbc0038I Pass   7: suminf.    0.00876 (1) obj. 0.317791 iterations 32\n",
      "Cbc0038I Pass   8: suminf.    0.18897 (1) obj. 0.317791 iterations 16\n",
      "Cbc0038I Pass   9: suminf.    0.00876 (1) obj. 0.317791 iterations 19\n",
      "Cbc0038I Pass  10: suminf.    0.00876 (1) obj. 0.317791 iterations 57\n",
      "Cbc0038I Pass  11: suminf.    0.18897 (1) obj. 0.317791 iterations 7\n",
      "Cbc0038I Pass  12: suminf.    0.00876 (1) obj. 0.317791 iterations 7\n",
      "Cbc0038I Pass  13: suminf.    0.00876 (1) obj. 0.317791 iterations 5\n",
      "Cbc0038I Pass  14: suminf.    0.18897 (1) obj. 0.317791 iterations 7\n",
      "Cbc0038I Pass  15: suminf.    0.00876 (1) obj. 0.317791 iterations 7\n",
      "Cbc0038I Pass  16: suminf.    0.00876 (1) obj. 0.317791 iterations 10\n",
      "Cbc0038I Pass  17: suminf.    0.18897 (1) obj. 0.317791 iterations 9\n",
      "Cbc0038I Pass  18: suminf.    0.00876 (1) obj. 0.317791 iterations 8\n",
      "Cbc0038I Pass  19: suminf.    0.00876 (1) obj. 0.317791 iterations 22\n",
      "Cbc0038I Pass  20: suminf.    0.18897 (1) obj. 0.317791 iterations 6\n",
      "Cbc0038I Pass  21: suminf.    0.00876 (1) obj. 0.317791 iterations 9\n",
      "Cbc0038I Pass  22: suminf.    0.00876 (1) obj. 0.317791 iterations 17\n",
      "Cbc0038I Pass  23: suminf.    0.18897 (1) obj. 0.317791 iterations 6\n",
      "Cbc0038I Pass  24: suminf.    0.00876 (1) obj. 0.317791 iterations 5\n",
      "Cbc0038I Pass  25: suminf.    0.00876 (1) obj. 0.317791 iterations 10\n",
      "Cbc0038I Pass  26: suminf.    0.18897 (1) obj. 0.317791 iterations 6\n",
      "Cbc0038I Pass  27: suminf.    0.00876 (1) obj. 0.317791 iterations 5\n",
      "Cbc0038I Pass  28: suminf.    0.00876 (1) obj. 0.317791 iterations 30\n",
      "Cbc0038I Pass  29: suminf.    0.18897 (1) obj. 0.317791 iterations 5\n",
      "Cbc0038I Pass  30: suminf.    0.00876 (1) obj. 0.317791 iterations 6\n",
      "Cbc0038I Pass  31: suminf.    0.00876 (1) obj. 0.317791 iterations 3\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 1 integers at bound fixed and 47 continuous\n",
      "Cbc0038I Full problem 166 rows 105 columns, reduced to 48 rows 27 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.02 seconds)\n",
      "Cbc0038I After 0.02 seconds - Feasibility pump exiting with objective of 0.317969 - took 0.01 seconds\n",
      "Cbc0012I Integer solution of 0.31796885 found by feasibility pump after 0 iterations and 0 nodes (0.02 seconds)\n",
      "Cbc0038I Full problem 166 rows 105 columns, reduced to 48 rows 27 columns\n",
      "Cbc0031I 3 added rows had average density of 3.3333333\n",
      "Cbc0013I At root node, 31 cuts changed objective from 0.31628066 to 0.31796885 in 1 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 19 row cuts average 3.0 elements, 1 column cuts (1 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 3 row cuts average 8.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 3 row cuts average 3.3 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 6 row cuts average 6.2 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0001I Search completed - best objective 0.3179688539269278, took 17 iterations and 0 nodes (0.02 seconds)\n",
      "Cbc0035I Maximum depth 0, 0 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.316281 to 0.317969\n",
      "Probing was tried 1 times and created 20 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 1 times and created 3 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 1 times and created 3 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 1 times and created 6 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.31796885\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               17\n",
      "Time (CPU seconds):             0.02\n",
      "Time (Wallclock seconds):       0.02\n",
      "\n",
      "Total time (CPU seconds):       0.03   (Wallclock seconds):       0.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create pyomo model\n",
    "m1 = pyo.ConcreteModel()\n",
    "\n",
    "# create an OMLT block for the neural network and build its formulation\n",
    "m1.nn = OmltBlock()\n",
    "\n",
    "# build formulation in block m.nn\n",
    "gnn_with_fixed_graph(m1.nn, nn1, N, A, scaled_input_bounds=input_bounds)\n",
    "\n",
    "# set the objective as the single output of the model\n",
    "m1.obj = pyo.Objective(expr=m1.nn.outputs[0])\n",
    "\n",
    "# solve the optimization problem\n",
    "status = pyo.SolverFactory(\"cbc\").solve(m1, tee=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the solution in original model to verify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31796885]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "edges = []\n",
    "for u in range(N):\n",
    "    for v in range(N):\n",
    "        if u != v and pyo.value(m1.nn.A[u, v]):\n",
    "            edges.append((u, v))\n",
    "for i in range(6):\n",
    "    X.append(pyo.value(m1.nn.inputs[i]))\n",
    "X = np.array(X).reshape(3, 2)\n",
    "edges = np.transpose(np.array(edges)).reshape(2, -1)\n",
    "nn.eval()\n",
    "print(nn1(torch.tensor(X).float(), torch.tensor(edges)).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Optimizing a GNN with Non-Fixed Graph\n",
    "\n",
    "Since GCN is not supported when the input graph is not fixed, we define a SAGE in `torch_geometric` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Linear, ReLU, Sigmoid\n",
    "from torch_geometric.nn import Sequential, SAGEConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from omlt.io.torch_geometric import gnn_with_non_fixed_graph\n",
    "\n",
    "import pyomo.environ as pyo\n",
    "from omlt import OmltBlock\n",
    "\n",
    "\n",
    "def SAGE_Sequential(activation, pooling):\n",
    "    torch.manual_seed(123)\n",
    "    return Sequential(\n",
    "        \"x, edge_index\",\n",
    "        [\n",
    "            (SAGEConv(2, 4, aggr=\"sum\"), \"x, edge_index -> x\"),\n",
    "            activation(),\n",
    "            (SAGEConv(4, 4, aggr=\"sum\"), \"x, edge_index -> x\"),\n",
    "            activation(),\n",
    "            Linear(4, 4),\n",
    "            (pooling, \"x, None -> x\"),\n",
    "            Linear(4, 2),\n",
    "            activation(),\n",
    "            Linear(2, 1),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the same procedure as in Example 1 except that $A$ is no longer needed for `gnn_with_non_fixed_graph`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.10 \n",
      "Build Date: Aug  1 2023 \n",
      "\n",
      "command line - /rds/general/user/sz421/home/anaconda3/envs/OMLT_test/bin/cbc -printingOptions all -import /var/tmp/pbs.8152010.pbs/tmpnzmuzmeh.pyomo.lp -stat=1 -solve -solu /var/tmp/pbs.8152010.pbs/tmpnzmuzmeh.pyomo.soln (default strategy 1)\n",
      "Option for printingOptions changed from normal to all\n",
      "Presolve 260 (-137) rows, 141 (-51) columns and 876 (-197) elements\n",
      "Statistics for presolved model\n",
      "Original problem has 32 integers (32 of which binary)\n",
      "Presolved problem has 29 integers (29 of which binary)\n",
      "==== 139 zero objective 3 different\n",
      "139 variables have objective of 0\n",
      "1 variables have objective of 0.203177\n",
      "1 variables have objective of 0.686721\n",
      "==== absolute objective values 3 different\n",
      "139 variables have objective of 0\n",
      "1 variables have objective of 0.203177\n",
      "1 variables have objective of 0.686721\n",
      "==== for integers 29 zero objective 1 different\n",
      "29 variables have objective of 0\n",
      "==== for integers absolute objective values 1 different\n",
      "29 variables have objective of 0\n",
      "===== end objective counts\n",
      "\n",
      "\n",
      "Problem has 260 rows, 141 columns (2 with objective) and 876 elements\n",
      "Column breakdown:\n",
      "0 of type 0.0->inf, 62 of type 0.0->up, 0 of type lo->inf, \n",
      "50 of type lo->up, 0 of type free, 0 of type fixed, \n",
      "0 of type -inf->0.0, 0 of type -inf->up, 29 of type 0.0->1.0 \n",
      "Row breakdown:\n",
      "0 of type E 0.0, 0 of type E 1.0, 0 of type E -1.0, \n",
      "26 of type E other, 0 of type G 0.0, 0 of type G 1.0, \n",
      "0 of type G other, 130 of type L 0.0, 24 of type L 1.0, \n",
      "80 of type L other, 0 of type Range 0.0->1.0, 0 of type Range other, \n",
      "0 of type Free \n",
      "Continuous objective value is 0.107106 - 0.00 seconds\n",
      "Cgl0003I 0 fixed, 0 tightened bounds, 4 strengthened rows, 0 substitutions\n",
      "Cgl0004I processed model has 237 rows, 118 columns (29 integer (29 of which binary)) and 989 elements\n",
      "Cbc0038I Initial state - 17 integers unsatisfied sum - 3.14435\n",
      "Cbc0038I Pass   1: suminf.    1.01765 (9) obj. 0.107106 iterations 71\n",
      "Cbc0038I Solution found of 0.107106\n",
      "Cbc0038I Relaxing continuous gives 0.107106\n",
      "Cbc0038I Before mini branch and bound, 12 integers at bound fixed and 38 continuous\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 0.107106 - took 0.00 seconds\n",
      "Cbc0012I Integer solution of 0.10710584 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0001I Search completed - best objective 0.1071058437228203, took 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0035I Maximum depth 0, 0 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.107106 to 0.107106\n",
      "Probing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.10710584\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.01\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define a GAGE sequential model\n",
    "nn2 = SAGE_Sequential(ReLU, global_add_pool)\n",
    "# number of nodes\n",
    "N = 3\n",
    "\n",
    "# size of inputs = number of nodes x number of input features\n",
    "input_size = [6]\n",
    "# define lower and upper bounds for each input\n",
    "input_bounds = {}\n",
    "for i in range(input_size[0]):\n",
    "    input_bounds[(i)] = (-1.0, 1.0)\n",
    "\n",
    "# create pyomo model\n",
    "m2 = pyo.ConcreteModel()\n",
    "\n",
    "# create an OMLT block for the neural network and build its formulation\n",
    "m2.nn = OmltBlock()\n",
    "\n",
    "# build formulation in block m.nn\n",
    "gnn_with_non_fixed_graph(m2.nn, nn2, N, scaled_input_bounds=input_bounds)\n",
    "\n",
    "# set the objective as the single output of the model\n",
    "m2.obj = pyo.Objective(expr=m2.nn.outputs[0])\n",
    "\n",
    "# solve the optimization problem\n",
    "status = pyo.SolverFactory(\"cbc\").solve(m2, tee=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For smooth activation function like Sigmoid, a smooth optimization solvers (such as Ipopt) is needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ipopt 3.14.12: \n",
      "==> Warning: Treating 6 binary and 0 integer variables as continuous.\n",
      "\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.14.12, running with linear solver MUMPS 5.2.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:      449\n",
      "Number of nonzeros in inequality constraint Jacobian.:      468\n",
      "Number of nonzeros in Lagrangian Hessian.............:       26\n",
      "\n",
      "Total number of variables............................:      166\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:      164\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:      103\n",
      "Total number of inequality constraints...............:      216\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:      216\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  0.0000000e+00 6.52e-01 7.52e-04  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  1.4746296e-02 6.32e-01 1.39e+00  -1.0 8.66e-01    -  1.28e-02 3.08e-02f  1\n",
      "   2  3.1873491e-01 2.12e-01 1.91e+01  -1.0 1.08e+00    -  3.20e-02 6.65e-01f  1\n",
      "   3  3.3291931e-01 1.93e-01 1.56e+01  -1.0 3.34e-01    -  7.50e-01 9.04e-02f  1\n",
      "   4  4.1605860e-01 8.12e-02 6.77e+00  -1.0 3.72e-01    -  5.03e-01 5.79e-01f  1\n",
      "   5  4.4666096e-01 4.00e-02 1.79e+01  -1.0 3.96e-01    -  7.08e-01 5.07e-01h  1\n",
      "   6  4.6317765e-01 1.82e-02 8.24e+01  -1.0 2.77e-01    -  1.00e+00 5.44e-01h  1\n",
      "   7  4.7137936e-01 7.43e-03 1.79e+02  -1.0 1.71e-01    -  1.00e+00 5.93e-01h  1\n",
      "   8  4.7469933e-01 3.08e-03 4.39e+02  -1.0 6.74e-02    -  1.00e+00 5.85e-01h  1\n",
      "   9  4.7608122e-01 1.28e-03 1.06e+03  -1.0 2.81e-02    -  1.00e+00 5.86e-01h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  4.7665353e-01 5.29e-04 2.55e+03  -1.0 1.16e-02    -  1.00e+00 5.86e-01h  1\n",
      "  11  4.7689072e-01 2.19e-04 6.15e+03  -1.0 4.81e-03    -  1.00e+00 5.86e-01h  1\n",
      "  12  4.7698895e-01 9.04e-05 1.48e+04  -1.0 1.99e-03    -  1.00e+00 5.87e-01h  1\n",
      "  13  4.7702965e-01 3.72e-05 3.54e+04  -1.0 8.22e-04    -  1.00e+00 5.88e-01h  1\n",
      "  14  4.7703175e-01 3.44e-05 1.90e+05  -1.0 3.38e-04    -  1.00e+00 7.40e-02f  4\n",
      "  15  4.7705226e-01 7.65e-06 8.24e+04  -1.0 3.13e-04    -  1.00e+00 7.78e-01h  1\n",
      "  16  4.7705268e-01 7.10e-06 7.71e+05  -1.0 6.96e-05    -  1.00e+00 7.20e-02f  4\n",
      "  17  4.7705708e-01 1.35e-06 2.87e+05  -1.0 6.46e-05    -  1.00e+00 8.10e-01h  1\n",
      "  18  4.7705717e-01 1.24e-06 2.66e+06  -1.0 1.23e-05    -  1.00e+00 8.39e-02f  4\n",
      "  19  4.7705812e-01 1.73e-12 1.00e-06  -1.0 1.13e-05    -  1.00e+00 1.00e+00h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20  4.7704910e-01 6.57e-09 9.15e+03  -5.7 5.47e-04    -  9.99e-01 1.00e+00f  1\n",
      "  21  4.7241100e-01 2.07e-03 4.56e+03  -5.7 2.94e-01    -  5.74e-01 9.31e-01f  1\n",
      "  22  4.7107356e-01 1.09e-03 1.39e+03  -5.7 1.83e-01    -  6.98e-01 8.54e-01h  1\n",
      "  23  4.7049903e-01 3.12e-03 2.62e+02  -5.7 3.65e-01    -  8.13e-01 8.89e-01f  1\n",
      "  24  4.7022019e-01 3.61e-03 3.33e+01  -5.7 5.62e-01    -  8.71e-01 5.90e-01f  1\n",
      "  25  4.7016027e-01 4.04e-04 7.29e-01  -5.7 2.34e-01    -  9.79e-01 1.00e+00f  1\n",
      "  26  4.7014863e-01 2.24e-05 7.68e-08  -5.7 3.49e-02    -  1.00e+00 1.00e+00h  1\n",
      "  27  4.7014848e-01 2.14e-08 1.85e-11  -5.7 8.68e-04    -  1.00e+00 1.00e+00h  1\n",
      "  28  4.7004983e-01 2.73e-04 3.48e+00  -8.6 1.95e-01    -  8.70e-01 8.51e-01h  1\n",
      "  29  4.7002727e-01 1.61e-04 1.42e-01  -8.6 7.82e-02    -  9.74e-01 9.83e-01h  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  30  4.7002699e-01 1.53e-06 2.93e-09  -8.6 7.19e-03    -  1.00e+00 1.00e+00f  1\n",
      "  31  4.7002699e-01 1.63e-10 2.98e-13  -8.6 7.38e-05    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 31\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   4.7002698793134651e-01    4.7002698793134651e-01\n",
      "Dual infeasibility......:   2.9843906102589463e-13    2.9843906102589463e-13\n",
      "Constraint violation....:   1.6278367542810201e-10    1.6278367542810201e-10\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.5060961140067687e-09    2.5060961140067687e-09\n",
      "Overall NLP error.......:   2.5060961140067687e-09    2.5060961140067687e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 41\n",
      "Number of objective gradient evaluations             = 32\n",
      "Number of equality constraint evaluations            = 41\n",
      "Number of inequality constraint evaluations          = 41\n",
      "Number of equality constraint Jacobian evaluations   = 32\n",
      "Number of inequality constraint Jacobian evaluations = 32\n",
      "Number of Lagrangian Hessian evaluations             = 31\n",
      "Total seconds in IPOPT                               = 0.065\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "\b"
     ]
    }
   ],
   "source": [
    "# define a GAGE sequential model\n",
    "nn3 = SAGE_Sequential(Sigmoid, global_add_pool)\n",
    "# number of nodes\n",
    "N = 3\n",
    "\n",
    "# size of inputs = number of nodes x number of input features\n",
    "input_size = [6]\n",
    "# define lower and upper bounds for each input\n",
    "input_bounds = {}\n",
    "for i in range(input_size[0]):\n",
    "    input_bounds[(i)] = (-1.0, 1.0)\n",
    "\n",
    "# create pyomo model\n",
    "m3 = pyo.ConcreteModel()\n",
    "\n",
    "# create an OMLT block for the neural network and build its formulation\n",
    "m3.nn = OmltBlock()\n",
    "\n",
    "# build formulation in block m.nn\n",
    "gnn_with_non_fixed_graph(m3.nn, nn3, N, scaled_input_bounds=input_bounds)\n",
    "\n",
    "# set the objective as the single output of the model\n",
    "m3.obj = pyo.Objective(expr=m3.nn.outputs[0])\n",
    "\n",
    "# solve the optimization problem\n",
    "status = pyo.SolverFactory(\"ipopt\").solve(m3, tee=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:OMLT_test]",
   "language": "python",
   "name": "conda-env-OMLT_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
